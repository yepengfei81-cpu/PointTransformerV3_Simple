import numpy as np
import open3d as o3d
import torch
from pathlib import Path
import json
from tqdm import tqdm
import argparse
import shutil

class PTv3DatasetGenerator:
    """
    PTv3 æ•°æ®é›†ç”Ÿæˆå™¨ï¼ˆæ”¯æŒæ··åˆæå–æ–¹æ³• + å…¨å±€å½’ä¸€åŒ–ï¼‰
    """
    
    def __init__(self, input_dir, output_dir, category_name, samples_per_bigpcd=200, 
                 radius=0.01, method='sphere', sphere_samples=None, cube_samples=None):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.category_name = category_name
        self.samples_per_bigpcd = samples_per_bigpcd
        self.radius = radius
        self.method = method
        
        # ğŸ”¥ æ··åˆæå–æ¨¡å¼
        self.sphere_samples = sphere_samples
        self.cube_samples = cube_samples
        
        # ğŸ”¥ æ–°å¢ï¼šå…¨å±€å½’ä¸€åŒ–å‚æ•°ï¼ˆæ­¥éª¤1ä¼šè®¡ç®—ï¼‰
        self.global_min = None
        self.global_max = None
        self.global_range = None
        
        # å¦‚æœæŒ‡å®šäº†æ··åˆæ¨¡å¼ï¼Œæ£€æŸ¥å‚æ•°
        if sphere_samples is not None and cube_samples is not None:
            self.mixed_mode = True
            self.samples_per_bigpcd = sphere_samples + cube_samples
            print(f"ğŸ”„ æ··åˆæå–æ¨¡å¼:")
            print(f"   çƒä½“æ–¹æ³•: {sphere_samples} ä¸ªæ ·æœ¬/å¤§ç‚¹äº‘")
            print(f"   ç«‹æ–¹ä½“æ–¹æ³•: {cube_samples} ä¸ªæ ·æœ¬/å¤§ç‚¹äº‘")
            print(f"   æ€»è®¡: {self.samples_per_bigpcd} ä¸ªæ ·æœ¬/å¤§ç‚¹äº‘")
        else:
            self.mixed_mode = False
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        self.category_dir = self.output_dir / category_name
        self.patches_dir = self.category_dir / "patches"
        self.patches_dir.mkdir(exist_ok=True, parents=True)
        
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ·ï¸  ç±»åˆ«åç§°: {self.category_name}")
    
    def _compute_global_normalization(self, ply_files):
        """
        ğŸŒ æ‰«ææ‰€æœ‰çˆ¶ç‚¹äº‘ï¼Œè®¡ç®—å…¨å±€è¾¹ç•Œ
        """
        print("\n" + "="*70)
        print("ğŸŒ æ­¥éª¤1ï¼šè®¡ç®—å…¨å±€å½’ä¸€åŒ–å‚æ•°")
        print("="*70)
        
        global_min = np.array([np.inf, np.inf, np.inf], dtype=np.float32)
        global_max = np.array([-np.inf, -np.inf, -np.inf], dtype=np.float32)
        
        print(f"ğŸ“‚ æ‰«æ {len(ply_files)} ä¸ªç‚¹äº‘æ–‡ä»¶...")
        
        for ply_file in tqdm(ply_files, desc="   æ‰«æç‚¹äº‘"):
            try:
                pcd = o3d.io.read_point_cloud(str(ply_file))
                coord = np.asarray(pcd.points, dtype=np.float32)
                
                if len(coord) == 0:
                    print(f"   âš ï¸  {ply_file.name} ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ›´æ–°å…¨å±€è¾¹ç•Œ
                global_min = np.minimum(global_min, coord.min(axis=0))
                global_max = np.maximum(global_max, coord.max(axis=0))
                
            except Exception as e:
                print(f"   âš ï¸  åŠ è½½ {ply_file.name} å¤±è´¥: {e}")
                continue
        
        # ğŸ”¥ è®¡ç®—å…¨å±€èŒƒå›´
        global_range = global_max - global_min
        
        # ğŸ”¥ ä¿å­˜åˆ°å®ä¾‹å˜é‡
        self.global_min = global_min
        self.global_max = global_max
        self.global_range = global_range  # (3,)
        
        print("\nâœ… å…¨å±€å½’ä¸€åŒ–å‚æ•°è®¡ç®—å®Œæˆ:")
        print(f"   global_min:   [{global_min[0]:.6f}, {global_min[1]:.6f}, {global_min[2]:.6f}]")
        print(f"   global_max:   [{global_max[0]:.6f}, {global_max[1]:.6f}, {global_max[2]:.6f}]")
        print(f"   global_range: [{global_range[0]:.6f}, {global_range[1]:.6f}, {global_range[2]:.6f}]")  # ğŸ”¥ æ‰“å° range
        print("="*70 + "\n")
    
    def _load_big_pointcloud(self, pcd_path):
        """åŠ è½½å¤§ç‚¹äº‘å¹¶æå–ç‰¹å¾"""
        pcd = o3d.io.read_point_cloud(str(pcd_path))
        
        if len(pcd.points) == 0:
            raise ValueError(f"ç‚¹äº‘ä¸ºç©º: {pcd_path}")
        
        global_coord = np.asarray(pcd.points).astype(np.float32)
        
        if pcd.has_colors():
            global_color = np.asarray(pcd.colors).astype(np.float32)
        else:
            global_color = np.ones((len(global_coord), 3), dtype=np.float32) * 0.5
            print(f"âš ï¸ {pcd_path.name} æ²¡æœ‰é¢œè‰²ï¼Œä½¿ç”¨é»˜è®¤ç°è‰²")
        
        # è®¡ç®—å•ä¸ªç‚¹äº‘çš„å½’ä¸€åŒ–å‚æ•°ï¼ˆç”¨äºåŠå¾„è°ƒæ•´ï¼‰
        pcd_min = global_coord.min(axis=0)
        pcd_max = global_coord.max(axis=0)
        pcd_size = pcd_max - pcd_min
        
        pcd_info = {
            'min': pcd_min,
            'max': pcd_max,
            'center': global_coord.mean(axis=0),
            'size': pcd_size,
            'pcd': pcd,
            'points': global_coord,
        }
        
        return global_coord, global_color, pcd_info
    
    def _is_region_valid(self, center, radius, pcd_info, method='sphere', 
                        min_points_ratio=0.5, coverage_threshold=0.5):
        """æ£€æŸ¥æå–åŒºåŸŸæ˜¯å¦æœ‰æ•ˆ"""
        center = np.array(center).reshape(3)
        points = pcd_info['points']
        
        # è¾¹ç•Œæ£€æŸ¥
        margin = radius * 1.1
        if np.any(center - margin < pcd_info['min']) or np.any(center + margin > pcd_info['max']):
            return False, 0
        
        # ç‚¹å¯†åº¦æ£€æŸ¥
        if method == 'sphere':
            distances = np.linalg.norm(points - center, axis=1)
            num_points = np.sum(distances < radius)
        else:  # cube
            diff = np.abs(points - center)
            mask = np.all(diff < radius, axis=1)
            num_points = np.sum(mask)
        
        # ä¼°ç®—ç†è®ºç‚¹æ•°
        total_points = len(points)
        cloud_volume = np.prod(pcd_info['size'])
        point_density = total_points / cloud_volume if cloud_volume > 0 else 0
        
        if method == 'sphere':
            region_volume = (4/3) * np.pi * (radius ** 3)
        else:
            region_volume = (2 * radius) ** 3
        
        expected_points = point_density * region_volume * coverage_threshold
        is_valid = num_points >= max(expected_points * min_points_ratio, 50)
        
        return is_valid, num_points
    
    def _find_valid_radius(self, pcd_info, initial_radius, method='sphere', max_attempts=5):
        """è‡ªåŠ¨å¯»æ‰¾åˆé€‚çš„æå–åŠå¾„"""
        pcd_size = pcd_info['size']
        min_size = np.min(pcd_size)
        
        # æ£€æŸ¥åˆå§‹åŠå¾„
        if initial_radius * 2.4 > min_size:
            adjusted_radius = min_size * 0.3
            print(f"âš ï¸ åˆå§‹åŠå¾„ {initial_radius:.6f} å¤ªå¤§ï¼Œè‡ªåŠ¨è°ƒæ•´ä¸º: {adjusted_radius:.6f}")
            return adjusted_radius
        
        radius = initial_radius
        
        for attempt in range(max_attempts):
            safe_margin = radius * 1.2
            safe_min = pcd_info['min'] + safe_margin
            safe_max = pcd_info['max'] - safe_margin
            
            if np.any(safe_min >= safe_max):
                radius = radius * 0.7
                print(f"   å°è¯• {attempt+1}: è°ƒæ•´åŠå¾„ä¸º {radius:.6f}")
                continue
            
            test_center = np.random.uniform(safe_min, safe_max)
            is_valid, num_points = self._is_region_valid(test_center, radius, pcd_info, method)
            
            if is_valid:
                if attempt > 0:
                    print(f"   âœ… æ‰¾åˆ°åˆé€‚åŠå¾„: {radius:.6f} (è°ƒæ•´äº† {attempt} æ¬¡)")
                return radius
            else:
                radius = radius * 0.7
                print(f"   å°è¯• {attempt+1}: ç‚¹æ•°ä¸è¶³ ({num_points}), è°ƒæ•´åŠå¾„ä¸º {radius:.6f}")
        
        print(f"   âŒ æ— æ³•æ‰¾åˆ°åˆé€‚çš„åŠå¾„")
        return None
    
    def _extract_random_patch(self, global_coord, global_color, pcd_info, radius, method):
        """
        ğŸ”¥ æ­¥éª¤2ï¼šä»å¤§ç‚¹äº‘ä¸­éšæœºæå–ä¸€ä¸ªå°ç‚¹äº‘ï¼ˆä½¿ç”¨å…¨å±€å½’ä¸€åŒ–ï¼‰
        """
        max_attempts = 50
        
        safe_margin = radius * 1.2
        safe_min = pcd_info['min'] + safe_margin
        safe_max = pcd_info['max'] - safe_margin
        
        if np.any(safe_min >= safe_max):
            return None, None, None, None, None, None, None, False
        
        for attempt in range(max_attempts):
            center = np.random.uniform(safe_min, safe_max).astype(np.float32)
            
            # æ ¹æ® method å‚æ•°é€‰æ‹©æå–æ–¹å¼
            if method == 'sphere':
                distances = np.linalg.norm(global_coord - center, axis=1)
                mask = distances < radius
            else:  # cube
                diff = np.abs(global_coord - center)
                mask = np.all(diff < radius, axis=1)
            
            indices = np.where(mask)[0]
            
            if len(indices) < 50:
                continue
            
            local_points = global_coord[indices]
            local_colors = global_color[indices]
            
            actual_center = local_points.mean(axis=0).astype(np.float32)
            
            gt_position_normalized = (actual_center - self.global_min) / self.global_range
            local_coord_normalized = (local_points - self.global_min) / self.global_range

            # ç¡®ä¿ç±»å‹æ­£ç¡®
            gt_position_normalized = gt_position_normalized.astype(np.float32)
            local_coord_normalized = local_coord_normalized.astype(np.float32)

            local_coord_original = local_points.astype(np.float32)
            gt_position_original = actual_center.astype(np.float32)

            # è¿”å›æ—¶å¢åŠ åŸå§‹åæ ‡
            return (local_coord_normalized, local_colors.astype(np.float32), gt_position_normalized,
                    local_coord_original, gt_position_original, radius, method, True)
        
        return None, None, None, None, None, None, None, False
    
    def process_single_bigpcd(self, pcd_path, bigpcd_id, global_sample_id_start, category_id):
        """å¤„ç†å•ä¸ªå¤§ç‚¹äº‘"""
        print(f"\nğŸ“‚ å¤„ç†: {pcd_path.name}")
        global_coord, global_color, pcd_info = self._load_big_pointcloud(pcd_path)
        print(f"   ç‚¹æ•°: {len(global_coord)}")
        print(f"   èŒƒå›´: X[{pcd_info['size'][0]:.6f}] Y[{pcd_info['size'][1]:.6f}] Z[{pcd_info['size'][2]:.6f}]")
        
        # ğŸ”¥ æ··åˆæ¨¡å¼ï¼šä¸ºçƒä½“å’Œç«‹æ–¹ä½“åˆ†åˆ«æ‰¾åˆé€‚çš„åŠå¾„
        if self.mixed_mode:
            print(f"\n   ğŸ”„ æ··åˆæå–æ¨¡å¼")
            
            # çƒä½“åŠå¾„
            print(f"   âšª çƒä½“æ–¹æ³•:")
            sphere_radius = self._find_valid_radius(pcd_info, self.radius, method='sphere')
            if sphere_radius is None:
                print(f"   âŒ çƒä½“æ–¹æ³•æ— æ³•æ‰¾åˆ°åˆé€‚åŠå¾„")
                sphere_radius = 0
            
            # ç«‹æ–¹ä½“åŠå¾„
            print(f"   ğŸŸ¦ ç«‹æ–¹ä½“æ–¹æ³•:")
            cube_radius = self._find_valid_radius(pcd_info, self.radius, method='cube')
            if cube_radius is None:
                print(f"   âŒ ç«‹æ–¹ä½“æ–¹æ³•æ— æ³•æ‰¾åˆ°åˆé€‚åŠå¾„")
                cube_radius = 0
            
            if sphere_radius == 0 and cube_radius == 0:
                print(f"   âŒ ä¸¤ç§æ–¹æ³•éƒ½æ— æ³•æ‰¾åˆ°åˆé€‚åŠå¾„ï¼Œè·³è¿‡")
                return [], global_sample_id_start
            
            # ğŸ”¥ ç”Ÿæˆæå–è®¡åˆ’
            extraction_plan = []
            
            if sphere_radius > 0:
                extraction_plan.extend([('sphere', sphere_radius)] * self.sphere_samples)
            
            if cube_radius > 0:
                extraction_plan.extend([('cube', cube_radius)] * self.cube_samples)
            
            # æ‰“ä¹±é¡ºåºï¼ˆå¯é€‰ï¼‰
            import random
            random.shuffle(extraction_plan)
            
        else:
            # å•ä¸€æ¨¡å¼
            adjusted_radius = self._find_valid_radius(pcd_info, self.radius, self.method)
            
            if adjusted_radius is None:
                print(f"   âŒ æ— æ³•æ‰¾åˆ°åˆé€‚çš„æå–åŠå¾„ï¼Œè·³è¿‡")
                return [], global_sample_id_start
            
            if abs(adjusted_radius - self.radius) > 1e-6:
                print(f"   ğŸ“ ä½¿ç”¨è°ƒæ•´åçš„åŠå¾„: {adjusted_radius:.6f}")
            
            extraction_plan = [(self.method, adjusted_radius)] * self.samples_per_bigpcd
        
        # ç”Ÿæˆæ ·æœ¬
        samples = []
        success_count = 0
        failed_count = 0
        sphere_count = 0
        cube_count = 0
        current_sample_id = global_sample_id_start
        
        pbar = tqdm(extraction_plan, desc=f"   æå–æ ·æœ¬", leave=False)
        
        parent_id_str = f"{bigpcd_id:03d}"  # "001", "002", ...
        
        for i, (method, radius) in enumerate(pbar):
            # ğŸ”¥ ä½¿ç”¨æŒ‡å®šçš„æ–¹æ³•å’ŒåŠå¾„æå–ï¼ˆæ­¥éª¤2ä¼šç”¨å…¨å±€å‚æ•°å½’ä¸€åŒ–ï¼‰
            (local_coord, local_color, gt_position,
            local_coord_original, gt_position_original,
            actual_radius, used_method, success) = self._extract_random_patch(global_coord, global_color, pcd_info, radius, method)
            
            if not success:
                failed_count += 1
                pbar.set_postfix({
                    'æˆåŠŸ': success_count, 
                    'å¤±è´¥': failed_count,
                    'âšªçƒ': sphere_count,
                    'ğŸŸ¦æ–¹': cube_count
                })
                continue
            
            # ç»Ÿè®¡
            if used_method == 'sphere':
                sphere_count += 1
            else:
                cube_count += 1
            
            # ğŸ”¥ æ­¥éª¤2ï¼šä¿å­˜æ—¶ä½¿ç”¨å…¨å±€å½’ä¸€åŒ–å‚æ•°
            data_dict = {
                "local_coord": local_coord,  # å·²ç”¨å…¨å±€å‚æ•°å½’ä¸€åŒ–
                "local_color": local_color,
                "gt_position": gt_position,  # å·²ç”¨å…¨å±€å‚æ•°å½’ä¸€åŒ–
                
                # ğŸ”¥ ä¿å­˜å…¨å±€å½’ä¸€åŒ–å‚æ•°ï¼ˆæ‰€æœ‰æ ·æœ¬ç›¸åŒï¼‰
                "norm_offset": self.global_min,   # å…¨å±€ min
                "norm_scale": self.global_range,  # å…¨å±€ scale

                "local_coord_original": local_coord_original,
                "gt_position_original": gt_position_original,      
                
                # ä¿ç•™å•ä¸ªç‚¹äº‘å‚æ•°ï¼ˆç”¨äºè°ƒè¯•å’ŒéªŒè¯ï¼‰
                "pcd_min": pcd_info['min'],
                "pcd_max": pcd_info['max'],
                "pcd_size": pcd_info['size'],
                
                # æå–æ–¹æ³•
                "extraction_method": used_method,
                "extraction_radius": float(actual_radius),
                
                # å…ƒä¿¡æ¯
                "category": self.category_name,
                "category_id": category_id,
                "bigpcd_name": pcd_path.name,
                "bigpcd_id": bigpcd_id,
                "parent_id": parent_id_str,
                "sample_id": current_sample_id,
                "name": f"{self.category_name}_{parent_id_str}_{used_method[0]}{i:05d}",
            }

            # æ–‡ä»¶ååŒ…å« parent_id
            output_filename = f"patch_{current_sample_id:06d}.pth"
            output_path = self.patches_dir / output_filename

            torch.save(data_dict, output_path)
            
            samples.append({
                'sample_id': current_sample_id,
                'name': data_dict['name'],
                'category': self.category_name,
                'method': used_method,
                'num_local_points': len(local_coord),
            })
            
            success_count += 1
            current_sample_id += 1
            
            pbar.set_postfix({
                'æˆåŠŸ': success_count, 
                'å¤±è´¥': failed_count,
                'âšªçƒ': sphere_count,
                'ğŸŸ¦æ–¹': cube_count
            })
        
        if success_count > 0:
            print(f"   âœ… ç”Ÿæˆ {success_count} ä¸ªæ ·æœ¬ (å¤±è´¥: {failed_count})")
            if self.mixed_mode:
                print(f"      âšª çƒä½“: {sphere_count} ä¸ª")
                print(f"      ğŸŸ¦ ç«‹æ–¹ä½“: {cube_count} ä¸ª")
        else:
            print(f"   âŒ è¯¥ç‚¹äº‘æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ ·æœ¬")
        
        return samples, current_sample_id
    
    def generate_dataset(self, category_id=0):
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†"""
        print(f"\n{'='*70}")
        print(f"ğŸš€ ç”Ÿæˆ PTv3 è®­ç»ƒæ•°æ®é›†ï¼ˆå…¨å±€å½’ä¸€åŒ–ç‰ˆæœ¬ï¼‰")
        print(f"{'='*70}")
        print(f"   è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   ç±»åˆ«åç§°: {self.category_name}")
        
        if self.mixed_mode:
            print(f"   ğŸ”„ æ··åˆæå–æ¨¡å¼:")
            print(f"      çƒä½“æ ·æœ¬: {self.sphere_samples}/å¤§ç‚¹äº‘")
            print(f"      ç«‹æ–¹ä½“æ ·æœ¬: {self.cube_samples}/å¤§ç‚¹äº‘")
            print(f"      æ€»è®¡: {self.samples_per_bigpcd}/å¤§ç‚¹äº‘")
        else:
            print(f"   æ¯ä¸ªå¤§ç‚¹äº‘ç”Ÿæˆ: {self.samples_per_bigpcd} ä¸ªæ ·æœ¬")
            print(f"   æå–æ–¹æ³•: {self.method}")
        
        print(f"   åˆå§‹æå–åŠå¾„: {self.radius}")
        print(f"{'='*70}\n")
        
        # æŸ¥æ‰¾æ‰€æœ‰ .ply æ–‡ä»¶
        ply_files = sorted(self.input_dir.glob("*.ply"))
        
        if len(ply_files) == 0:
            print(f"âŒ åœ¨ {self.input_dir} ä¸­æœªæ‰¾åˆ° .ply æ–‡ä»¶")
            return None
        
        print(f"ğŸ“‚ å‘ç° {len(ply_files)} ä¸ªç‚¹äº‘æ–‡ä»¶:")
        for f in ply_files:
            print(f"   - {f.name}")
        
        # ğŸ”¥ æ­¥éª¤1ï¼šè®¡ç®—å…¨å±€å½’ä¸€åŒ–å‚æ•°
        self._compute_global_normalization(ply_files)
        
        # å¤„ç†æ¯ä¸ªå¤§ç‚¹äº‘
        all_samples = []
        global_sample_id = 0
        
        for bigpcd_id, pcd_path in enumerate(ply_files, start=1):
            samples, global_sample_id = self.process_single_bigpcd(
                pcd_path, bigpcd_id, global_sample_id, category_id
            )
            all_samples.extend(samples)
        
        # ç»Ÿè®¡
        sphere_total = sum(1 for s in all_samples if s['method'] == 'sphere')
        cube_total = sum(1 for s in all_samples if s['method'] == 'cube')
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'total_samples': len(all_samples),
            'category': self.category_name,
            'category_id': category_id,
            'num_bigpcds': len(ply_files),
            'samples_per_bigpcd': self.samples_per_bigpcd,
            'initial_radius': float(self.radius),
            
            # ğŸ”¥ ä¿å­˜å…¨å±€å½’ä¸€åŒ–å‚æ•°
            'global_normalization': {
                'global_min': self.global_min.tolist(),
                'global_max': self.global_max.tolist(),
                'global_range': self.global_range.tolist(),  # ğŸ”¥ ä¿å­˜ range è€Œä¸æ˜¯ scale
            }
        }
        
        if self.mixed_mode:
            dataset_info['mixed_mode'] = True
            dataset_info['sphere_samples_per_bigpcd'] = self.sphere_samples
            dataset_info['cube_samples_per_bigpcd'] = self.cube_samples
            dataset_info['sphere_total'] = sphere_total
            dataset_info['cube_total'] = cube_total
        else:
            dataset_info['mixed_mode'] = False
            dataset_info['method'] = self.method
        
        info_path = self.category_dir / "category_info.json"
        with open(info_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
        print(f"{'='*70}")
        print(f"âœ… æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        
        if self.mixed_mode:
            print(f"   âšª çƒä½“æ ·æœ¬: {sphere_total}")
            print(f"   ğŸŸ¦ ç«‹æ–¹ä½“æ ·æœ¬: {cube_total}")
        
        print(f"\nğŸŒ å…¨å±€å½’ä¸€åŒ–å‚æ•°:")
        print(f"   global_min:   {self.global_min}")
        print(f"   global_max:   {self.global_max}")
        print(f"   global_range: {self.global_range}")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.category_dir}")
        print(f"ğŸ“ æ ·æœ¬ç›®å½•: {self.patches_dir}")
        print(f"ğŸ“„ ä¿¡æ¯æ–‡ä»¶: {info_path}")
        print(f"{'='*70}\n")
        
        return dataset_info


def merge_categories(output_dir, categories):
    """åˆå¹¶å¤šä¸ªç±»åˆ«çš„æ•°æ®é›†ä¿¡æ¯"""
    output_dir = Path(output_dir)
    
    print(f"\n{'='*70}")
    print(f"ğŸ“¦ åˆå¹¶æ•°æ®é›†ä¿¡æ¯")
    print(f"{'='*70}\n")
    
    category_to_id = {cat: i for i, cat in enumerate(sorted(categories))}
    
    all_info = {
        'categories': list(category_to_id.keys()),
        'category_to_id': category_to_id,
        'samples_by_category': {},
        'total_samples': 0,
    }
    
    for category in categories:
        category_dir = output_dir / category
        info_path = category_dir / "category_info.json"
        
        if not info_path.exists():
            print(f"âš ï¸  {category}: ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        with open(info_path) as f:
            cat_info = json.load(f)
        
        all_info['samples_by_category'][category] = cat_info['total_samples']
        all_info['total_samples'] += cat_info['total_samples']
        
        print(f"âœ… {category:15s}: {cat_info['total_samples']:5d} æ ·æœ¬")
    
    # ä¿å­˜å…¨å±€ä¿¡æ¯
    global_info_path = output_dir / "dataset_info.json"
    with open(global_info_path, 'w') as f:
        json.dump(all_info, f, indent=2)
    
    print(f"\nğŸ“„ å…¨å±€ä¿¡æ¯å·²ä¿å­˜: {global_info_path}")
    print(f"âœ… æ€»æ ·æœ¬æ•°: {all_info['total_samples']}")
    print(f"{'='*70}\n")
    
    return all_info


def verify_single_sample(pth_path):
    """éªŒè¯å•ä¸ªæ ·æœ¬"""
    print(f"\n{'='*70}")
    print(f"ğŸ” éªŒè¯æ ·æœ¬: {pth_path}")
    print(f"{'='*70}\n")
    
    data_dict = torch.load(pth_path, weights_only=False)
    
    required_keys = [
        "local_coord", "local_color", "gt_position",
        "norm_offset", "norm_scale",  # ğŸ”¥ æ–°å¢æ£€æŸ¥
        "pcd_min", "pcd_max", "pcd_size",
        "extraction_method", "extraction_radius",
        "category", "category_id", "bigpcd_name", "bigpcd_id", "parent_id", "sample_id", "name"
    ]
    
    print("å­—æ®µæ£€æŸ¥:")
    for key in required_keys:
        if key in data_dict:
            value = data_dict[key]
            if isinstance(value, np.ndarray):
                print(f"   âœ… {key:20s} shape={str(value.shape):15s} dtype={value.dtype}")
            else:
                print(f"   âœ… {key:20s} value={value}")
        else:
            print(f"   âš ï¸  {key:20s} ç¼ºå¤±")
    
    print(f"\næ•°æ®èŒƒå›´:")
    print(f"   Local coord:  [{data_dict['local_coord'].min():.4f}, {data_dict['local_coord'].max():.4f}]")
    print(f"   Local color:  [{data_dict['local_color'].min():.4f}, {data_dict['local_color'].max():.4f}]")
    
    gt_pos = data_dict['gt_position']
    print(f"   GT position:  [{gt_pos[0]:.6f}, {gt_pos[1]:.6f}, {gt_pos[2]:.6f}]")
    
    if 'extraction_method' in data_dict:
        print(f"\næå–æ–¹æ³•:")
        print(f"   Method: {data_dict['extraction_method']}")
        print(f"   Radius: {data_dict['extraction_radius']:.6f}")
    
    # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºå…¨å±€å½’ä¸€åŒ–å‚æ•°
    if 'norm_offset' in data_dict and 'norm_scale' in data_dict:
        print(f"\nğŸŒ å…¨å±€å½’ä¸€åŒ–å‚æ•°:")
        norm_offset = data_dict['norm_offset']
        norm_scale = data_dict['norm_scale']
        
        print(f"   norm_offset (global_min): {norm_offset}")
        
        # ğŸ”¥ æ£€æŸ¥ norm_scale çš„å½¢çŠ¶
        if isinstance(norm_scale, np.ndarray):
            if norm_scale.shape == (3,):
                print(f"   norm_scale (global_range): {norm_scale}")  # âœ… æ­£ç¡®
    
    if 'pcd_min' in data_dict:
        print(f"\nğŸ“¦ å•ä¸ªç‚¹äº‘å‚æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰:")
        print(f"   pcd_min:  {data_dict['pcd_min']}")
        print(f"   pcd_max:  {data_dict['pcd_max']}")
        print(f"   pcd_size: {data_dict['pcd_size']}")
    
    print(f"\nåæ ‡èŒƒå›´æ£€æŸ¥:")
    local_min = data_dict['local_coord'].min()
    local_max = data_dict['local_coord'].max()
    gt_min = gt_pos.min()
    gt_max = gt_pos.max()
    
    if 0 <= local_min and local_max <= 1:
        print(f"   âœ… å°ç‚¹äº‘åæ ‡åœ¨ [0, 1] èŒƒå›´å†…")
    else:
        print(f"   âš ï¸  å°ç‚¹äº‘åæ ‡è¶…å‡º [0, 1] èŒƒå›´: [{local_min:.4f}, {local_max:.4f}]")
    
    if 0 <= gt_min and gt_max <= 1:
        print(f"   âœ… GT ä½ç½®åœ¨ [0, 1] èŒƒå›´å†…")
    else:
        print(f"   âš ï¸  GT ä½ç½®è¶…å‡º [0, 1] èŒƒå›´: [{gt_min:.4f}, {gt_max:.4f}]")
    
    local_center = data_dict['local_coord'].mean(axis=0)
    distance_to_gt = np.linalg.norm(local_center - gt_pos)
    print(f"\nè´¨å¿ƒæ£€æŸ¥:")
    print(f"   å°ç‚¹äº‘è´¨å¿ƒ: [{local_center[0]:.4f}, {local_center[1]:.4f}, {local_center[2]:.4f}]")
    print(f"   GT ä½ç½®:    [{gt_pos[0]:.4f}, {gt_pos[1]:.4f}, {gt_pos[2]:.4f}]")
    print(f"   è·ç¦»:       {distance_to_gt:.6f}")
    
    if distance_to_gt < 0.01:
        print(f"   âœ… è´¨å¿ƒä½ç½®åˆç†")
    else:
        print(f"   âš ï¸  è´¨å¿ƒè·ç¦» GT è¾ƒè¿œ")

    # ğŸ”¥ æ–°å¢ï¼šåå½’ä¸€åŒ–éªŒè¯
    if 'norm_offset' in data_dict and 'norm_scale' in data_dict:
        print(f"\n{'='*70}")
        print(f"ğŸ”„ åå½’ä¸€åŒ–éªŒè¯")
        print(f"{'='*70}")
        
        norm_offset = data_dict['norm_offset']
        norm_scale = data_dict['norm_scale']
        
        # åå½’ä¸€åŒ–å±€éƒ¨åæ ‡
        local_coord_normalized = data_dict['local_coord']
        local_coord_denormalized = local_coord_normalized * norm_scale + norm_offset
        
        # åå½’ä¸€åŒ– GT ä½ç½®
        gt_position_normalized = data_dict['gt_position']
        gt_position_denormalized = gt_position_normalized * norm_scale + norm_offset
        
        print(f"\nğŸ“Š åå½’ä¸€åŒ–ç»“æœ:")
        print(f"   å±€éƒ¨åæ ‡èŒƒå›´:")
        print(f"      å½’ä¸€åŒ–:   [{local_coord_normalized.min():.6f}, {local_coord_normalized.max():.6f}]")
        print(f"      åå½’ä¸€åŒ–: [{local_coord_denormalized.min():.6f}, {local_coord_denormalized.max():.6f}]")
        
        print(f"\n   GT ä½ç½®:")
        print(f"      å½’ä¸€åŒ–:   [{gt_position_normalized[0]:.6f}, {gt_position_normalized[1]:.6f}, {gt_position_normalized[2]:.6f}]")
        print(f"      åå½’ä¸€åŒ–: [{gt_position_denormalized[0]:.6f}, {gt_position_denormalized[1]:.6f}, {gt_position_denormalized[2]:.6f}]")
        
        # ğŸ”¥ æ£€æŸ¥åå½’ä¸€åŒ–åçš„è´¨å¿ƒ
        local_center_denormalized = local_coord_denormalized.mean(axis=0)
        distance_denormalized = np.linalg.norm(local_center_denormalized - gt_position_denormalized)
        
        print(f"\n   è´¨å¿ƒæ£€æŸ¥ï¼ˆåå½’ä¸€åŒ–åï¼‰:")
        print(f"      å±€éƒ¨è´¨å¿ƒ: [{local_center_denormalized[0]:.6f}, {local_center_denormalized[1]:.6f}, {local_center_denormalized[2]:.6f}]")
        print(f"      GT ä½ç½®:  [{gt_position_denormalized[0]:.6f}, {gt_position_denormalized[1]:.6f}, {gt_position_denormalized[2]:.6f}]")
        print(f"      è·ç¦»:     {distance_denormalized:.6f} ç±³ = {distance_denormalized*1000:.2f} æ¯«ç±³")
        
        if distance_denormalized < 0.001:  # 1mm ä»¥å†…
            print(f"      âœ… è´¨å¿ƒä½ç½®éå¸¸ç²¾ç¡®")
        elif distance_denormalized < 0.005:  # 5mm ä»¥å†…
            print(f"      âœ… è´¨å¿ƒä½ç½®åˆç†")
        else:
            print(f"      âš ï¸  è´¨å¿ƒè·ç¦» GT è¾ƒè¿œ")
        
        # ğŸ”¥ ä¸å•ä¸ªç‚¹äº‘å‚æ•°å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰
        if 'pcd_min' in data_dict and 'pcd_size' in data_dict:
            print(f"\n   ğŸ“¦ ä¸å•ä¸ªç‚¹äº‘å‚æ•°å¯¹æ¯”:")
            
            # ä½¿ç”¨å•ä¸ªç‚¹äº‘å‚æ•°åå½’ä¸€åŒ–
            pcd_min = data_dict['pcd_min']
            pcd_size = data_dict['pcd_size']
            
            local_coord_denorm_old = local_coord_normalized * pcd_size + pcd_min
            gt_position_denorm_old = gt_position_normalized * pcd_size + pcd_min
            
            print(f"      ä½¿ç”¨å•ä¸ªç‚¹äº‘å‚æ•°åå½’ä¸€åŒ–:")
            print(f"         å±€éƒ¨åæ ‡èŒƒå›´: [{local_coord_denorm_old.min():.6f}, {local_coord_denorm_old.max():.6f}]")
            print(f"         GT ä½ç½®: [{gt_position_denorm_old[0]:.6f}, {gt_position_denorm_old[1]:.6f}, {gt_position_denorm_old[2]:.6f}]")
            
            # ğŸ”¥ æ£€æŸ¥å·®å¼‚
            coord_diff = np.abs(local_coord_denormalized - local_coord_denorm_old).max()
            gt_diff = np.linalg.norm(gt_position_denormalized - gt_position_denorm_old)
            
            print(f"\n      å·®å¼‚åˆ†æ:")
            print(f"         åæ ‡æœ€å¤§å·®å¼‚: {coord_diff:.6f} ç±³ = {coord_diff*1000:.2f} æ¯«ç±³")
            print(f"         GT ä½ç½®å·®å¼‚:  {gt_diff:.6f} ç±³ = {gt_diff*1000:.2f} æ¯«ç±³")
            
            if coord_diff < 1e-6 and gt_diff < 1e-6:
                print(f"         âœ… ä¸¤ç§å‚æ•°ç»“æœä¸€è‡´ï¼ˆè¯´æ˜æ˜¯åŒä¸€ä¸ªçˆ¶ç‚¹äº‘ï¼‰")
            else:
                print(f"         âš ï¸  ä¸¤ç§å‚æ•°ç»“æœä¸åŒï¼ˆè¯´æ˜ä½¿ç”¨äº†ä¸åŒçš„å½’ä¸€åŒ–ç©ºé—´ï¼‰")
        
        print(f"{'='*70}\n")

    print(f"\n{'='*70}")
    print(f"âœ… éªŒè¯å®Œæˆ")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ PTv3 è®­ç»ƒæ•°æ®é›†ï¼ˆæ”¯æŒæ··åˆæå– + å…¨å±€å½’ä¸€åŒ–ï¼‰")
    
    parser.add_argument("--input_dir", type=str, default=None,
                       help="è¾“å…¥ç›®å½•ï¼ˆåŒ…å«åŸå§‹ .ply æ–‡ä»¶ï¼‰")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="è¾“å‡ºæ ¹ç›®å½•")
    parser.add_argument("--category", type=str, default=None,
                       help="ç±»åˆ«åç§°ï¼ˆä¾‹å¦‚ Scissors, Cup, Avocadoï¼‰")
    parser.add_argument("--category_id", type=int, default=0,
                       help="ç±»åˆ« IDï¼ˆé»˜è®¤ 0ï¼‰")
    parser.add_argument("--samples_per_bigpcd", type=int, default=None,
                       help="æ¯ä¸ªå¤§ç‚¹äº‘æå–çš„æ ·æœ¬æ•°ï¼ˆå•ä¸€æ¨¡å¼ï¼‰")
    parser.add_argument("--radius", type=float, default=0.01,
                       help="åˆå§‹æå–åŠå¾„")
    parser.add_argument("--method", type=str, default="sphere",
                       choices=["sphere", "cube"],
                       help="æå–æ–¹æ³•ï¼ˆå•ä¸€æ¨¡å¼ï¼‰")
    
    # ğŸ”¥ æ··åˆæ¨¡å¼å‚æ•°
    parser.add_argument("--sphere_samples", type=int, default=None,
                       help="çƒä½“æ–¹æ³•æå–çš„æ ·æœ¬æ•°/å¤§ç‚¹äº‘ï¼ˆæ··åˆæ¨¡å¼ï¼‰")
    parser.add_argument("--cube_samples", type=int, default=None,
                       help="ç«‹æ–¹ä½“æ–¹æ³•æå–çš„æ ·æœ¬æ•°/å¤§ç‚¹äº‘ï¼ˆæ··åˆæ¨¡å¼ï¼‰")
    
    parser.add_argument("--verify", type=str, default=None,
                       help="éªŒè¯å•ä¸ªæ ·æœ¬")
    parser.add_argument("--merge", type=str, nargs='+', default=None,
                       help="åˆå¹¶å¤šä¸ªç±»åˆ«çš„ä¿¡æ¯ï¼Œä¾‹å¦‚: --merge Scissors Cup Avocado")
    parser.add_argument("--merge_dir", type=str, default=None,
                       help="åˆå¹¶æ—¶çš„è¾“å‡ºæ ¹ç›®å½•")
    
    args = parser.parse_args()
    
    if args.verify:
        # éªŒè¯æ¨¡å¼
        verify_single_sample(args.verify)
    
    elif args.merge and args.merge_dir:
        # åˆå¹¶æ¨¡å¼
        merge_categories(args.merge_dir, args.merge)
    
    elif args.input_dir and args.output_dir and args.category:
        # ğŸ”¥ æ£€æŸ¥æ¨¡å¼
        if args.sphere_samples is not None and args.cube_samples is not None:
            # æ··åˆæ¨¡å¼
            generator = PTv3DatasetGenerator(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                category_name=args.category,
                radius=args.radius,
                sphere_samples=args.sphere_samples,
                cube_samples=args.cube_samples
            )
        elif args.samples_per_bigpcd is not None:
            # å•ä¸€æ¨¡å¼
            generator = PTv3DatasetGenerator(
                input_dir=args.input_dir,
                output_dir=args.output_dir,
                category_name=args.category,
                samples_per_bigpcd=args.samples_per_bigpcd,
                radius=args.radius,
                method=args.method
            )
        else:
            parser.error("è¯·æŒ‡å®š --samples_per_bigpcdï¼ˆå•ä¸€æ¨¡å¼ï¼‰æˆ– --sphere_samples + --cube_samplesï¼ˆæ··åˆæ¨¡å¼ï¼‰")
            exit(1)
        
        generator.generate_dataset(category_id=args.category_id)
    
    else:
        parser.error("è¯·æŒ‡å®šæ­£ç¡®çš„å‚æ•°ç»„åˆ")
        print("\nä½¿ç”¨ç¤ºä¾‹:")
        print("\n1. å•ä¸€æ¨¡å¼ï¼ˆåªç”¨çƒä½“ï¼‰:")
        print("   python script.py --input_dir scans --output_dir data_root --category Scissors --samples_per_bigpcd 100 --method sphere")
        print("\n2. ğŸ”¥ æ··åˆæ¨¡å¼ï¼ˆçƒä½“50ä¸ª + ç«‹æ–¹ä½“50ä¸ªï¼‰:")
        print("   python script.py --input_dir scans --output_dir data_root --category Scissors --sphere_samples 50 --cube_samples 50")
        print("\n3. éªŒè¯æ ·æœ¬:")
        print("   python script.py --verify data_root/Scissors/patches/patch_001_000001.pth")
        print("\n4. åˆå¹¶ç±»åˆ«:")
        print("   python script.py --merge Scissors Cup Avocado --merge_dir data_root")