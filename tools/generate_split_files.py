import json
from pathlib import Path
import random

def generate_split_files(data_root, train_ratio=0.7, val_ratio=0.2):
    """
    ç”Ÿæˆ train.txt, val.txt, test.txt
    
    ç›®å½•ç»“æ„ï¼š
        data_root/
        â”œâ”€â”€ Scissors/patches/*.pth
        â”œâ”€â”€ Cup/patches/*.pth
        â””â”€â”€ Avocado/patches/*.pth
    
    ç”Ÿæˆæ–‡ä»¶ï¼š
        data_root/train.txt  # Scissors/patches/patch_000001.pth
        data_root/val.txt
        data_root/test.txt
    """
    data_root = Path(data_root)
    all_samples = []
    
    for category_dir in data_root.iterdir():
        if not category_dir.is_dir():
            continue
        
        category_name = category_dir.name
        if category_name in ['train.txt', 'val.txt', 'test.txt']:
            continue
        
        patches_dir = category_dir / "patches"
        if not patches_dir.exists():
            print(f"âš ï¸ è·³è¿‡ {category_name}ï¼ˆæ²¡æœ‰ patches ç›®å½•ï¼‰")
            continue
        
        # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
        category_samples = []
        for pth_file in sorted(patches_dir.glob("patch_*.pth")):
            # æ ¼å¼ï¼šScissors/patches/patch_000001.pth
            relative_path = pth_file.relative_to(data_root)
            category_samples.append(str(relative_path))
        
        print(f"ğŸ“¦ {category_name}: {len(category_samples)} æ ·æœ¬")
        all_samples.extend(category_samples)
    
    # æ‰“ä¹±
    random.seed(42)  # ğŸ”¥ å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    random.shuffle(all_samples)
    
    # åˆ’åˆ†
    n_total = len(all_samples)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)
    
    train_samples = all_samples[:n_train]
    val_samples = all_samples[n_train:n_train + n_val]
    test_samples = all_samples[n_train + n_val:]
    
    # ä¿å­˜
    with open(data_root / "train.txt", 'w') as f:
        f.write('\n'.join(train_samples))
    
    with open(data_root / "val.txt", 'w') as f:
        f.write('\n'.join(val_samples))
    
    with open(data_root / "test.txt", 'w') as f:
        f.write('\n'.join(test_samples))
    
    print(f"\nâœ… ç”Ÿæˆå®Œæˆï¼š")
    print(f"   ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
    print(f"   ğŸ“Š æ€»æ ·æœ¬æ•°: {n_total}")
    print(f"   ğŸ”¹ Train: {len(train_samples)} ({train_ratio*100:.0f}%)")
    print(f"   ğŸ”¹ Val:   {len(val_samples)} ({val_ratio*100:.0f}%)")
    print(f"   ğŸ”¹ Test:  {len(test_samples)} ({(1-train_ratio-val_ratio)*100:.0f}%)")
    
    # æ˜¾ç¤ºæ¯ä¸ªç±»åˆ«çš„åˆ†å¸ƒ
    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š")
    for split_name, split_samples in [("Train", train_samples), ("Val", val_samples), ("Test", test_samples)]:
        category_counts = {}
        for sample in split_samples:
            category = sample.split('/')[0]
            category_counts[category] = category_counts.get(category, 0) + 1
        print(f"   {split_name}: {category_counts}")


if __name__ == "__main__":
    generate_split_files(
        "/root/autodl-tmp/touch_processed_data/",  # ğŸ”¥ ä½ çš„æ•°æ®æ ¹ç›®å½•
        train_ratio=0.7,
        val_ratio=0.2
    )