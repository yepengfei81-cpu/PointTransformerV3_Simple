# tools/generate_split_files.py

import json
import torch
import numpy as np
from pathlib import Path
import random

# ğŸ”¥ å®šä¹‰æ­£ç¡®çš„ç±»åˆ«æ˜ å°„
CATEGORY_MAP = {
    "Scissors": 0,
    "Cup": 1,
    "Avocado": 2,
}

def fix_single_file(pth_file, correct_category_id):
    """
    ä¿®å¤å•ä¸ªæ–‡ä»¶çš„ category_id
    
    Args:
        pth_file: .pth æ–‡ä»¶è·¯å¾„
        correct_category_id: æ­£ç¡®çš„ç±»åˆ« ID
    
    Returns:
        bool: æ˜¯å¦ä¿®å¤æˆåŠŸ
    """
    try:
        # åŠ è½½æ•°æ®
        data = torch.load(pth_file, weights_only=False)
        
        # æ£€æŸ¥å¹¶ä¿®å¤ category_id
        if "category_id" not in data:
            data["category_id"] = correct_category_id
            torch.save(data, pth_file)
            return True
        
        old_id = data["category_id"]
        
        # å¤„ç†ä¸åŒç±»å‹
        if isinstance(old_id, np.ndarray):
            old_id = int(old_id.item())
        elif isinstance(old_id, (list, tuple)):
            old_id = int(old_id[0])
        else:
            old_id = int(old_id)
        
        # å¦‚æœä¸æ­£ç¡®ï¼Œä¿®å¤
        if old_id != correct_category_id:
            data["category_id"] = correct_category_id
            torch.save(data, pth_file)
            return True
        
        return False
        
    except Exception as e:
        print(f"   âŒ Error fixing {pth_file.name}: {e}")
        return False


def generate_split_files(data_root, train_ratio=0.7, val_ratio=0.2, fix_category_id=True):
    """
    ç”Ÿæˆ train.txt, val.txt, test.txtï¼Œå¹¶ä¿®å¤ category_id
    
    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        fix_category_id: æ˜¯å¦è‡ªåŠ¨ä¿®å¤ category_id
    
    ç›®å½•ç»“æ„ï¼š
        data_root/
        â”œâ”€â”€ Scissors/patches/*.pth
        â”œâ”€â”€ Cup/patches/*.pth
        â””â”€â”€ Avocado/patches/*.pth
    
    ç”Ÿæˆæ–‡ä»¶ï¼š
        data_root/train.txt  # Scissors/patches/patch_000001.pth
        data_root/val.txt
        data_root/test.txt
    """
    data_root = Path(data_root)
    
    # ğŸ”¥ åˆ é™¤æ—§çš„åˆ’åˆ†æ–‡ä»¶
    print("ğŸ—‘ï¸  åˆ é™¤æ—§çš„åˆ’åˆ†æ–‡ä»¶...")
    for split_file in ["train.txt", "val.txt", "test.txt"]:
        split_path = data_root / split_file
        if split_path.exists():
            split_path.unlink()
            print(f"   âœ… åˆ é™¤: {split_file}")
    
    print("\n" + "=" * 80)
    print("å¼€å§‹ç”Ÿæˆæ–°çš„åˆ’åˆ†æ–‡ä»¶")
    print("=" * 80)
    
    all_samples = []
    category_stats = {}
    
    # éå†æ¯ä¸ªç±»åˆ«
    for category_name, correct_category_id in CATEGORY_MAP.items():
        category_dir = data_root / category_name
        
        if not category_dir.exists():
            print(f"\nâš ï¸  è·³è¿‡ {category_name}ï¼ˆç›®å½•ä¸å­˜åœ¨ï¼‰")
            continue
        
        patches_dir = category_dir / "patches"
        if not patches_dir.exists():
            print(f"\nâš ï¸  è·³è¿‡ {category_name}ï¼ˆæ²¡æœ‰ patches ç›®å½•ï¼‰")
            continue
        
        print(f"\nğŸ“¦ å¤„ç† {category_name} (ID={correct_category_id})...")
        
        # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
        category_samples = []
        pth_files = sorted(patches_dir.glob("patch_*.pth"))
        
        if len(pth_files) == 0:
            print(f"   âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .pth æ–‡ä»¶")
            continue
        
        fixed_count = 0
        
        for pth_file in pth_files:
            # æ ¼å¼ï¼šScissors/patches/patch_000001.pth
            relative_path = pth_file.relative_to(data_root)
            category_samples.append(str(relative_path))
            
            # ğŸ”¥ ä¿®å¤ category_id
            if fix_category_id:
                if fix_single_file(pth_file, correct_category_id):
                    fixed_count += 1
        
        print(f"   ğŸ“Š æ ·æœ¬æ•°: {len(category_samples)}")
        if fix_category_id:
            print(f"   ğŸ”§ ä¿®å¤æ•°: {fixed_count}")
        
        all_samples.extend(category_samples)
        category_stats[category_name] = len(category_samples)
    
    if len(all_samples) == 0:
        print("\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ·æœ¬ï¼")
        return
    
    # ğŸ”¥ æ‰“ä¹±ï¼ˆå›ºå®šéšæœºç§å­ï¼‰
    print(f"\nğŸ”€ æ‰“ä¹±æ ·æœ¬...")
    random.seed(42)
    random.shuffle(all_samples)
    
    # ğŸ”¥ åˆ’åˆ†
    n_total = len(all_samples)
    n_train = int(n_total * train_ratio)
    n_val = int(n_total * val_ratio)
    
    train_samples = all_samples[:n_train]
    val_samples = all_samples[n_train:n_train + n_val]
    test_samples = all_samples[n_train + n_val:]
    
    # ğŸ”¥ ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜åˆ’åˆ†æ–‡ä»¶...")
    
    with open(data_root / "train.txt", 'w') as f:
        f.write('\n'.join(train_samples))
    print(f"   âœ… train.txt ({len(train_samples)} æ ·æœ¬)")
    
    with open(data_root / "val.txt", 'w') as f:
        f.write('\n'.join(val_samples))
    print(f"   âœ… val.txt ({len(val_samples)} æ ·æœ¬)")
    
    with open(data_root / "test.txt", 'w') as f:
        f.write('\n'.join(test_samples))
    print(f"   âœ… test.txt ({len(test_samples)} æ ·æœ¬)")
    
    # ğŸ”¥ ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("âœ… ç”Ÿæˆå®Œæˆ")
    print("=" * 80)
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
    print(f"   ğŸ“ æ•°æ®æ ¹ç›®å½•: {data_root}")
    print(f"   ğŸ“Š æ€»æ ·æœ¬æ•°: {n_total}")
    print(f"   ğŸ”¹ Train: {len(train_samples):4d} ({len(train_samples)/n_total*100:5.1f}%)")
    print(f"   ğŸ”¹ Val:   {len(val_samples):4d} ({len(val_samples)/n_total*100:5.1f}%)")
    print(f"   ğŸ”¹ Test:  {len(test_samples):4d} ({len(test_samples)/n_total*100:5.1f}%)")
    
    # ğŸ”¥ æ¯ä¸ªç±»åˆ«çš„åˆ†å¸ƒ
    print(f"\nğŸ“Š å„ç±»åˆ«æ ·æœ¬æ•°ï¼š")
    for category_name, count in category_stats.items():
        print(f"   {category_name:10s}: {count:4d} æ ·æœ¬")
    
    print(f"\nğŸ“Š å„åˆ’åˆ†çš„ç±»åˆ«åˆ†å¸ƒï¼š")
    for split_name, split_samples in [
        ("Train", train_samples),
        ("Val", val_samples),
        ("Test", test_samples)
    ]:
        category_counts = {}
        for sample in split_samples:
            category = sample.split('/')[0]
            category_counts[category] = category_counts.get(category, 0) + 1
        
        print(f"\n   {split_name}:")
        for category_name in CATEGORY_MAP.keys():
            count = category_counts.get(category_name, 0)
            if count > 0:
                percentage = count / len(split_samples) * 100
                print(f"      {category_name:10s}: {count:4d} ({percentage:5.1f}%)")


def verify_splits(data_root):
    """
    éªŒè¯ç”Ÿæˆçš„åˆ’åˆ†æ–‡ä»¶
    
    æ£€æŸ¥ï¼š
    1. æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    2. æ ·æœ¬æ•°é‡
    3. category_id æ˜¯å¦æ­£ç¡®
    """
    data_root = Path(data_root)
    
    print("\n" + "=" * 80)
    print("éªŒè¯åˆ’åˆ†æ–‡ä»¶")
    print("=" * 80)
    
    for split_name in ["train", "val", "test"]:
        split_file = data_root / f"{split_name}.txt"
        
        if not split_file.exists():
            print(f"\nâŒ {split_name}.txt ä¸å­˜åœ¨")
            continue
        
        print(f"\nğŸ“‹ éªŒè¯ {split_name}.txt...")
        
        # è¯»å–æ–‡ä»¶åˆ—è¡¨
        with open(split_file, 'r') as f:
            file_list = [line.strip() for line in f if line.strip()]
        
        print(f"   æ ·æœ¬æ•°: {len(file_list)}")
        
        # æ£€æŸ¥å‰3ä¸ªæ ·æœ¬çš„ category_id
        print(f"   æ£€æŸ¥å‰3ä¸ªæ ·æœ¬:")
        for i, rel_path in enumerate(file_list[:3]):
            full_path = data_root / rel_path
            
            if not full_path.exists():
                print(f"      {i+1}. âŒ æ–‡ä»¶ä¸å­˜åœ¨: {rel_path}")
                continue
            
            try:
                data = torch.load(full_path, weights_only=False)
                category_id = data.get("category_id", -1)
                
                # ä»è·¯å¾„æ¨æ–­æœŸæœ›çš„ç±»åˆ«
                category_name = rel_path.split('/')[0]
                expected_id = CATEGORY_MAP.get(category_name, -1)
                
                status = "âœ…" if category_id == expected_id else "âŒ"
                cat_name = ["Scissors", "Cup", "Avocado"][category_id] if 0 <= category_id < 3 else "Unknown"
                
                print(f"      {i+1}. {status} {rel_path}")
                print(f"          category_id={category_id} ({cat_name}), expected={expected_id}")
                
            except Exception as e:
                print(f"      {i+1}. âŒ åŠ è½½å¤±è´¥: {rel_path}")
                print(f"          Error: {e}")
    
    print("\n" + "=" * 80)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate train/val/test split files")
    parser.add_argument(
        "--data_root",
        type=str,
        default="/root/autodl-tmp/touch_processed_data/",
        help="Path to dataset root",
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.7,
        help="Training set ratio (default: 0.7)",
    )
    parser.add_argument(
        "--val_ratio",
        type=float,
        default=0.2,
        help="Validation set ratio (default: 0.2)",
    )
    parser.add_argument(
        "--fix_category_id",
        action="store_true",
        default=True,
        help="Fix category_id while generating splits (default: True)",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Verify splits after generation",
    )
    
    args = parser.parse_args()
    
    # ç”Ÿæˆåˆ’åˆ†æ–‡ä»¶
    generate_split_files(
        data_root=args.data_root,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        fix_category_id=args.fix_category_id,
    )
    
    # éªŒè¯
    if args.verify:
        verify_splits(args.data_root)