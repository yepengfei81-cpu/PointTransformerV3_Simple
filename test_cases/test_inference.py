import sys
from pathlib import Path
import torch
import numpy as np
import open3d as o3d
import argparse
import json
import csv
from typing import Dict, Tuple, Optional

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pointcept.models import build_model
from pointcept.utils.config import Config


def extract_sample_id_from_patch_name(patch_name: str) -> Optional[int]:
    """ä» patch æ–‡ä»¶åæå–æ ·æœ¬ ID"""
    patch_name = Path(patch_name).stem
    
    if patch_name.startswith('patch_'):
        try:
            return int(patch_name.split('_')[1])
        except:
            pass
    
    try:
        return int(patch_name.split('_')[-1])
    except:
        return None


class PTv3ContactMatcher:
    """PTv3 æ¥è§¦ç‚¹ä½ç½®é¢„æµ‹å™¨"""
    
    def __init__(self, config_path: str, checkpoint_path: str, device=None):
        self.config_path = Path(config_path)
        self.checkpoint_path = Path(checkpoint_path)
        
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        
        print(f"ğŸ”§ åŠ è½½é…ç½®: {self.config_path}")
        self.cfg = Config.fromfile(str(self.config_path))
        
        print(f"ğŸ”§ æ„å»ºæ¨¡å‹...")
        self.model = build_model(self.cfg.model).to(self.device)
        
        print(f"ğŸ”§ åŠ è½½æƒé‡: {self.checkpoint_path}")
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device, weights_only=False)
        
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
        
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith('module.'):
                new_state_dict[k[7:]] = v
            else:
                new_state_dict[k] = v
        
        self.model.load_state_dict(new_state_dict)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        if 'epoch' in checkpoint:
            print(f"   è®­ç»ƒè½®æ•°: {checkpoint['epoch']}")
        if 'best_metric_value' in checkpoint:
            print(f"   æœ€ä½³æŒ‡æ ‡: {checkpoint['best_metric_value']:.6f}")
        print(f"   æ¨ç†è®¾å¤‡: {self.device}")
    
    def predict(self, input_dict: Dict, verbose: bool = False) -> np.ndarray:
        model_input = {
            'local': {},
            'parent': {},
        }
        
        if 'local' not in input_dict:
            raise KeyError("Missing 'local' key in input_dict")
        
        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¶ç‚¹äº‘
        use_parent = hasattr(self.model, 'use_parent_cloud') and self.model.use_parent_cloud
        has_parent = 'parent' in input_dict and len(input_dict['parent']) > 0
        
        if use_parent and not has_parent:
            raise ValueError(
                "æ¨¡å‹éœ€è¦çˆ¶ç‚¹äº‘ç‰¹å¾ï¼ˆuse_parent_cloud=Trueï¼‰ï¼Œä½†æœªæä¾›çˆ¶ç‚¹äº‘æ•°æ®ã€‚\n"
                "è¯·ç¡®ä¿è½¬æ¢è„šæœ¬ä¸­æŒ‡å®šäº† --bigpcd_id å‚æ•°ã€‚"
            )
        
        # å¤„ç†å±€éƒ¨ç‚¹äº‘
        local_data = input_dict['local']
        num_local_points = None
        for key in ['coord', 'feat', 'grid_coord']:
            if key not in local_data:
                raise KeyError(f"Missing '{key}' in input_dict['local']")
            
            value = local_data[key]
            
            if isinstance(value, np.ndarray):
                value = torch.from_numpy(value)
            elif not isinstance(value, torch.Tensor):
                value = torch.tensor(value)
            
            if num_local_points is None:
                num_local_points = value.shape[0]

            if key in ['coord', 'feat']:
                value = value.float()
            elif key == 'grid_coord':
                value = value.long()
            
            model_input['local'][key] = value.to(self.device) 

        # local batch and offset
        if 'batch' in local_data:
            batch = local_data['batch']
            if isinstance(batch, np.ndarray):
                batch = torch.from_numpy(batch).long()
            elif not isinstance(batch, torch.Tensor):
                batch = torch.tensor(batch).long()
            model_input['local']['batch'] = batch.to(self.device)
        else:
            model_input['local']['batch'] = torch.zeros(num_local_points, dtype=torch.long, device=self.device)
        
        if 'offset' in local_data:
            offset = local_data['offset']
            if isinstance(offset, np.ndarray):
                offset = torch.from_numpy(offset).long()
            elif not isinstance(offset, torch.Tensor):
                offset = torch.tensor(offset).long()
            model_input['local']['offset'] = offset.to(self.device)
        else:
            model_input['local']['offset'] = torch.tensor([num_local_points], dtype=torch.long, device=self.device)
        
        # ğŸ”¥ å¤„ç†çˆ¶ç‚¹äº‘ï¼ˆåªæœ‰åœ¨éœ€è¦ä¸”å­˜åœ¨æ—¶æ‰å¤„ç†ï¼‰
        if use_parent and has_parent:
            parent_data = input_dict['parent']
            num_parent_points = None
            
            for key in ['coord', 'feat', 'grid_coord']:
                if key not in parent_data:
                    raise KeyError(f"Missing '{key}' in input_dict['parent']")
                
                value = parent_data[key]
                if isinstance(value, np.ndarray):
                    value = torch.from_numpy(value)
                elif not isinstance(value, torch.Tensor):
                    value = torch.tensor(value)

                if num_parent_points is None:
                    num_parent_points = value.shape[0]

                if key in ['coord', 'feat']:
                    value = value.float()
                elif key == 'grid_coord':
                    value = value.long()
                
                model_input['parent'][key] = value.to(self.device)
            
            # parent batch and offset
            if 'batch' in parent_data:
                batch = parent_data['batch']
                if isinstance(batch, np.ndarray):
                    batch = torch.from_numpy(batch).long()
                elif not isinstance(batch, torch.Tensor):
                    batch = torch.tensor(batch).long()
                model_input['parent']['batch'] = batch.to(self.device)
            else:
                model_input['parent']['batch'] = torch.zeros(num_parent_points, dtype=torch.long, device=self.device)
            
            if 'offset' in parent_data:
                offset = parent_data['offset']
                if isinstance(offset, np.ndarray):
                    offset = torch.from_numpy(offset).long()
                elif not isinstance(offset, torch.Tensor):
                    offset = torch.tensor(offset).long()
                model_input['parent']['offset'] = offset.to(self.device)
            else:
                model_input['parent']['offset'] = torch.tensor([num_parent_points], dtype=torch.long, device=self.device)

        # å¤„ç†å…¨å±€å‚æ•°
        if 'grid_size' in input_dict:
            grid_size = input_dict['grid_size']
            if isinstance(grid_size, (int, float)):
                grid_size = torch.tensor(grid_size, dtype=torch.float32)
            elif isinstance(grid_size, np.ndarray):
                grid_size = torch.from_numpy(grid_size).float()
            elif not isinstance(grid_size, torch.Tensor):
                grid_size = torch.tensor(grid_size).float()
            model_input['grid_size'] = grid_size.to(self.device)

        if 'category_id' in input_dict:
            category_id = input_dict['category_id']
            
            if isinstance(category_id, np.ndarray):
                category_id = torch.from_numpy(category_id).long()
            elif isinstance(category_id, (int, np.integer)):
                category_id = torch.tensor(category_id, dtype=torch.long)
            elif not isinstance(category_id, torch.Tensor):
                category_id = torch.tensor(category_id).long()

            if category_id.dim() == 0:
                category_id = category_id.unsqueeze(0)
            
            model_input['category_id'] = category_id.to(self.device)

        # ğŸ”¥ æ‰“å°è°ƒè¯•ä¿¡æ¯
        if verbose:
            print("\nğŸ“Š æ¨¡å‹è¾“å…¥:")
            print(f"   use_parent_cloud: {use_parent}")
            print(f"   fusion_type: {getattr(self.model, 'fusion_type', 'N/A')}")
            
            print(f"\n   å±€éƒ¨ç‚¹äº‘:")
            for key, value in model_input['local'].items():
                if isinstance(value, torch.Tensor):
                    print(f"      {key}: shape={value.shape}, dtype={value.dtype}")
            
            if use_parent and has_parent:
                print(f"\n   çˆ¶ç‚¹äº‘:")
                for key, value in model_input['parent'].items():
                    if isinstance(value, torch.Tensor):
                        print(f"      {key}: shape={value.shape}, dtype={value.dtype}")
            else:
                print(f"\n   çˆ¶ç‚¹äº‘: æ— ")
            
            print(f"\n   å…¨å±€å‚æ•°:")
            if 'grid_size' in model_input:
                print(f"      grid_size: {model_input['grid_size'].item()}")
            if 'category_id' in model_input:
                print(f"      category_id: {model_input['category_id'].item()}")
            
            if 'norm_offset' in input_dict and 'norm_scale' in input_dict:
                print(f"\nğŸ“ å½’ä¸€åŒ–å‚æ•°:")
                norm_offset = input_dict['norm_offset']
                norm_scale = input_dict['norm_scale']
                if isinstance(norm_offset, torch.Tensor):
                    norm_offset = norm_offset.cpu().numpy()
                if isinstance(norm_scale, torch.Tensor):
                    norm_scale = norm_scale.cpu().numpy()
                print(f"   norm_offset: {norm_offset}")
                print(f"   norm_scale: {norm_scale}")

        # æ¨ç†
        with torch.no_grad():
            output_dict = self.model(model_input)
        
        pred_position = output_dict['pred_position'].cpu().numpy()[0]
        
        return pred_position


def load_patch_data(
    patch_path: Path, 
    parent_pcd_root: Path,
    grid_size: float = 0.002,
    verbose: bool = False
) -> Dict[str, torch.Tensor]:
    """
    åŠ è½½åŸå§‹ patch æ•°æ®ï¼Œå¹¶è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    ğŸ”¥ æ”¯æŒæ—  GT å’Œæ— çˆ¶ç‚¹äº‘çš„çœŸå®æ¨ç†åœºæ™¯
    """
    try:
        data = torch.load(patch_path, map_location='cpu', weights_only=False)
    except Exception as e:
        raise RuntimeError(f"Failed to load {patch_path}: {e}")
    
    # âœ… æ£€æŸ¥å¿…éœ€å­—æ®µ
    required_keys = ['local_coord', 'local_color', 'norm_offset', 'norm_scale']
    missing_keys = [k for k in required_keys if k not in data]
    if missing_keys:
        raise KeyError(f"Missing required keys: {missing_keys}")
    
    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ GT
    gt_available = data.get('gt_available', False)
    
    # âœ… 1. æå–å±€éƒ¨ç‚¹äº‘æ•°æ®
    local_coord = data['local_coord']
    local_color = data['local_color']
    
    if isinstance(local_coord, np.ndarray):
        local_coord = torch.from_numpy(local_coord).float()
    elif not isinstance(local_coord, torch.Tensor):
        local_coord = torch.tensor(local_coord).float()
    
    if isinstance(local_color, np.ndarray):
        local_color = torch.from_numpy(local_color).float()
    elif not isinstance(local_color, torch.Tensor):
        local_color = torch.tensor(local_color).float()
    
    # âœ… 2. æå–å½’ä¸€åŒ–å‚æ•°
    norm_offset = data['norm_offset']
    norm_scale = data['norm_scale']
    
    if isinstance(norm_offset, np.ndarray):
        norm_offset = torch.from_numpy(norm_offset).float()
    elif not isinstance(norm_offset, torch.Tensor):
        norm_offset = torch.tensor(norm_offset).float()
    
    if isinstance(norm_scale, np.ndarray):
        norm_scale = torch.from_numpy(norm_scale).float()
    elif not isinstance(norm_scale, torch.Tensor):
        norm_scale = torch.tensor(norm_scale).float()
    
    # ğŸ”¥ 3. å°è¯•åŠ è½½çˆ¶ç‚¹äº‘
    bigpcd_id = data.get('bigpcd_id', data.get('parent_id'))
    category = data.get('category', 'Unknown')
    
    parent_pcd_path = None
    parent_coord_normalized = None
    parent_color = None
    
    if bigpcd_id is not None and bigpcd_id >= 0:
        if isinstance(bigpcd_id, (int, np.integer)):
            bigpcd_id_str = f"{bigpcd_id:03d}"
        else:
            bigpcd_id_str = str(bigpcd_id).zfill(3)
        
        bigpcd_name = data.get('bigpcd_name', f'bigpointcloud_{bigpcd_id_str}.ply')
        
        possible_paths = [
            parent_pcd_root / category / bigpcd_name,
            parent_pcd_root / category / f'bigpointcloud_{bigpcd_id_str}.ply',
            parent_pcd_root / category / f'data{bigpcd_id_str}.ply',
            parent_pcd_root / bigpcd_name,
        ]
        
        for path in possible_paths:
            if path.exists():
                parent_pcd_path = path
                break
        
        if parent_pcd_path is not None:
            if verbose:
                print(f"   ğŸ“‚ åŠ è½½çˆ¶ç‚¹äº‘: {parent_pcd_path}")
            
            import open3d as o3d
            parent_pcd = o3d.io.read_point_cloud(str(parent_pcd_path))
            parent_coord = np.asarray(parent_pcd.points).astype(np.float32)
            parent_color = np.asarray(parent_pcd.colors).astype(np.float32)
            
            parent_coord = torch.from_numpy(parent_coord).float()
            parent_color = torch.from_numpy(parent_color).float()
            
            # å½’ä¸€åŒ–çˆ¶ç‚¹äº‘
            parent_coord_normalized = (parent_coord - norm_offset) / norm_scale
        else:
            if verbose:
                print(f"   âš ï¸  æœªæ‰¾åˆ°çˆ¶ç‚¹äº‘ ID={bigpcd_id}ï¼ˆå°è¯•çš„è·¯å¾„ï¼š{possible_paths[0]}ï¼‰")
    else:
        if verbose:
            print(f"   âš ï¸  æœªæŒ‡å®šçˆ¶ç‚¹äº‘ IDï¼ˆbigpcd_id={bigpcd_id}ï¼‰ï¼Œè·³è¿‡çˆ¶ç‚¹äº‘åŠ è½½")
    
    # âœ… 4. å½’ä¸€åŒ–å±€éƒ¨ç‚¹äº‘
    local_coord_normalized = local_coord
    
    # âœ… 5. ä½“ç´ åŒ–
    local_grid_coord = torch.floor(local_coord_normalized / grid_size).long()
    
    # âœ… 6. GT ä½ç½®
    gt_position = None
    if 'gt_position' in data and gt_available:
        gt_position = data['gt_position']
        if isinstance(gt_position, np.ndarray):
            gt_position = torch.from_numpy(gt_position).float()
        elif not isinstance(gt_position, torch.Tensor):
            gt_position = torch.tensor(gt_position).float()
    
    # âœ… 7. category_id
    category_id = data.get('category_id')
    if category_id is not None:
        if isinstance(category_id, np.ndarray):
            category_id = torch.from_numpy(category_id).long()
        elif isinstance(category_id, (int, np.integer)):
            category_id = torch.tensor(category_id, dtype=torch.long)
        elif not isinstance(category_id, torch.Tensor):
            category_id = torch.tensor(category_id).long()
    
    # âœ… 8. æ„é€ ç»“æœå­—å…¸
    result = {
        'local': {
            'coord': local_coord_normalized,
            'feat': local_color,
            'grid_coord': local_grid_coord,
            'offset': torch.tensor([local_coord_normalized.shape[0]], dtype=torch.long),
        },
        'gt_position': gt_position,
        'gt_available': gt_available,
        'norm_offset': norm_offset,
        'norm_scale': norm_scale,
        'category_id': category_id,
        'name': data.get('name', patch_path.stem),
        'grid_size': grid_size,
        '_raw_data': data,
        '_parent_pcd_path': parent_pcd_path,
    }
    
    # ğŸ”¥ å¦‚æœæœ‰çˆ¶ç‚¹äº‘ï¼Œæ·»åŠ  parent å­—æ®µ
    if parent_coord_normalized is not None:
        parent_grid_coord = torch.floor(parent_coord_normalized / grid_size).long()
        result['parent'] = {
            'coord': parent_coord_normalized,
            'feat': parent_color,
            'grid_coord': parent_grid_coord,
            'offset': torch.tensor([parent_coord_normalized.shape[0]], dtype=torch.long),
            'name': str(parent_pcd_path.name) if parent_pcd_path else "unknown",
        }
    else:
        result['parent'] = {}
    
    if verbose:
        print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"   å±€éƒ¨ç‚¹äº‘: {result['local']['coord'].shape[0]} ç‚¹")
        if 'parent' in result and len(result['parent']) > 0:
            print(f"   çˆ¶ç‚¹äº‘: {result['parent']['coord'].shape[0]} ç‚¹")
        else:
            print(f"   çˆ¶ç‚¹äº‘: æ— ")
        print(f"   GT å¯ç”¨: {'æ˜¯' if gt_available else 'å¦'}")
        if gt_position is not None:
            print(f"   GT ä½ç½®ï¼ˆå½’ä¸€åŒ–ï¼‰: {gt_position.tolist()}")
        if category_id is not None:
            print(f"   ç±»åˆ« ID: {category_id.item()}")
    
    return result


def denormalize_position(position_normalized: np.ndarray, norm_offset, norm_scale) -> np.ndarray:
    """åå½’ä¸€åŒ–ä½ç½®"""
    if isinstance(norm_offset, torch.Tensor):
        norm_offset = norm_offset.cpu().numpy()
    if isinstance(norm_scale, torch.Tensor):
        norm_scale = norm_scale.cpu().numpy()
    
    if isinstance(position_normalized, torch.Tensor):
        position_normalized = position_normalized.cpu().numpy()
    if not isinstance(position_normalized, np.ndarray):
        position_normalized = np.array(position_normalized)
    
    return position_normalized * norm_scale + norm_offset


def visualize_prediction(
    patch_data: Dict,
    pred_position: np.ndarray,
    gt_position: Optional[np.ndarray] = None,  # ğŸ”¥ GT å¯ä»¥ä¸º None
    complete_model_path: Path = None,
    patch_name: str = "",
    save_dir: Path = None,
    show_window: bool = False
):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœ
    ğŸ”¥ æ”¯æŒæ—  GT çš„ç®€åŒ–å¯è§†åŒ–
    """
    geometries = []
    
    # ğŸ”¥ 1. åŠ è½½çˆ¶ç‚¹äº‘
    if complete_model_path is None:
        if '_parent_pcd_path' in patch_data:
            complete_model_path = patch_data['_parent_pcd_path']
        else:
            raise ValueError("ç¼ºå°‘çˆ¶ç‚¹äº‘è·¯å¾„")
    
    if not complete_model_path.exists():
        raise FileNotFoundError(f"çˆ¶ç‚¹äº‘ä¸å­˜åœ¨: {complete_model_path}")
    
    print(f"ğŸ“‚ åŠ è½½çˆ¶ç‚¹äº‘: {complete_model_path.name}")
    complete_pcd = o3d.io.read_point_cloud(str(complete_model_path))
    complete_points = np.asarray(complete_pcd.points)
    
    # ğŸ”¥ 2. åå½’ä¸€åŒ–å±€éƒ¨ç‚¹äº‘
    if '_raw_data' in patch_data:
        coord_normalized = patch_data['_raw_data']['local_coord']
        if isinstance(coord_normalized, torch.Tensor):
            coord_normalized = coord_normalized.cpu().numpy()
        elif not isinstance(coord_normalized, np.ndarray):
            coord_normalized = np.array(coord_normalized)
    else:
        coord_normalized = patch_data['local']['coord']
        if isinstance(coord_normalized, torch.Tensor):
            coord_normalized = coord_normalized.cpu().numpy()
    
    norm_offset = patch_data['norm_offset']
    norm_scale = patch_data['norm_scale']
    coord_real = denormalize_position(coord_normalized, norm_offset, norm_scale)
    
    # ğŸ”¥ 3. åˆ›å»ºå‡ ä½•ä½“
    complete_pcd.paint_uniform_color([0.85, 0.85, 0.85])
    geometries.append(complete_pcd)
    
    patch_pcd = o3d.geometry.PointCloud()
    patch_pcd.points = o3d.utility.Vector3dVector(coord_real)
    patch_pcd.paint_uniform_color([1.0, 0.65, 0.0])
    geometries.append(patch_pcd)
    
    # ğŸ”¥ 4. çƒä½“å’Œè¿æ¥çº¿
    parent_range = complete_points.max(axis=0) - complete_points.min(axis=0)
    pcd_size_complete = np.linalg.norm(parent_range)
    sphere_radius = pcd_size_complete * 0.01
    
    # ğŸ”¥ é¢„æµ‹çƒä½“ï¼ˆçº¢è‰²ï¼‰
    pred_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=sphere_radius)
    pred_sphere.translate(pred_position)
    pred_sphere.paint_uniform_color([1, 0, 0])
    pred_sphere.compute_vertex_normals()
    geometries.append(pred_sphere)
    
    # ğŸ”¥ GT çƒä½“ï¼ˆè“è‰²ï¼Œå¯é€‰ï¼‰
    error = None
    if gt_position is not None:
        gt_sphere = o3d.geometry.TriangleMesh.create_sphere(radius=sphere_radius)
        gt_sphere.translate(gt_position)
        gt_sphere.paint_uniform_color([0, 0, 1])
        gt_sphere.compute_vertex_normals()
        geometries.append(gt_sphere)
        
        # è¿æ¥çº¿
        line_set = o3d.geometry.LineSet()
        line_set.points = o3d.utility.Vector3dVector(np.array([pred_position, gt_position]))
        line_set.lines = o3d.utility.Vector2iVector([[0, 1]])
        line_set.colors = o3d.utility.Vector3dVector([[1, 1, 0]])
        geometries.append(line_set)
        
        error = np.linalg.norm(pred_position - gt_position)
    
    # åæ ‡ç³»
    pcd_center = (complete_points.min(axis=0) + complete_points.max(axis=0)) / 2
    coord_size = pcd_size_complete * 0.1
    coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
        size=coord_size, 
        origin=pcd_center
    )
    geometries.append(coord_frame)
    
    # ğŸ”¥ 5. æ‰“å°ä¿¡æ¯
    print(f"\n{'='*70}")
    print(f"ğŸ¨ å¯è§†åŒ– ({patch_name}):")
    print(f"   é¢„æµ‹ä½ç½®: [{pred_position[0]:.6f}, {pred_position[1]:.6f}, {pred_position[2]:.6f}] ç±³")
    if gt_position is not None:
        print(f"   GT ä½ç½®:   [{gt_position[0]:.6f}, {gt_position[1]:.6f}, {gt_position[2]:.6f}] ç±³")
        print(f"   è¯¯å·®:      {error:.6f} ç±³ = {error*1000:.2f} æ¯«ç±³")
    else:
        print(f"   GT ä½ç½®:   æ— ")
    print(f"{'='*70}\n")
    
    # ğŸ”¥ 6. ä¿å­˜ç»“æœ
    if save_dir is not None:
        save_dir = Path(save_dir)
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ PLY
        combined_pcd = o3d.geometry.PointCloud()
        combined_pcd += complete_pcd
        combined_pcd += patch_pcd
        combined_pcd += pred_sphere.sample_points_uniformly(number_of_points=1000)
        if gt_position is not None:
            combined_pcd += gt_sphere.sample_points_uniformly(number_of_points=1000)
        
        ply_path = save_dir / f"{patch_name}_visualization.ply"
        o3d.io.write_point_cloud(str(ply_path), combined_pcd)
        print(f"âœ… PLY æ–‡ä»¶å·²ä¿å­˜: {ply_path}")
        
        # ä¿å­˜ JSON
        result = {
            'patch_name': patch_name,
            'pred_position': pred_position.tolist(),
            'complete_model': str(complete_model_path),
        }
        if gt_position is not None:
            result['gt_position'] = gt_position.tolist()
            result['error_meters'] = float(error)
            result['error_mm'] = float(error * 1000)
        
        json_path = save_dir / f"{patch_name}_result.json"
        with open(json_path, 'w') as f:
            json.dump(result, f, indent=2)
        print(f"âœ… JSON å·²ä¿å­˜: {json_path}")
    
    # ğŸ”¥ 7. æ˜¾ç¤ºçª—å£
    if show_window:
        try:
            o3d.visualization.draw_geometries(
                geometries,
                window_name=f"æ¥è§¦ç‚¹é¢„æµ‹ - {patch_name}",
                width=1920,
                height=1080,
                point_show_normal=False
            )
        except Exception as e:
            print(f"âš ï¸  æ— æ³•æ˜¾ç¤ºçª—å£: {e}")


def test_single_patch(
    matcher: PTv3ContactMatcher,
    patch_path: Path,
    parent_pcd_root: Path,
    save_dir: Path = None,
    show_window: bool = True,
    grid_size: float = 0.002,
):
    """ğŸ”¥ å•æ ·æœ¬æ¨ç†ï¼ˆæ”¯æŒæ—  GT å’Œæ— çˆ¶ç‚¹äº‘ï¼‰"""
    print(f"\n{'='*80}")
    print(f"ğŸ”® å•æ ·æœ¬æ¨ç†æ¨¡å¼")
    print(f"{'='*80}")
    print(f"ğŸ“‚ æ ·æœ¬æ–‡ä»¶: {patch_path}")
    
    if not patch_path.exists():
        raise FileNotFoundError(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {patch_path}")
    
    try:
        # ğŸ”¥ åŠ è½½æ•°æ®
        print(f"\nâ³ åŠ è½½æ•°æ®...")
        patch_data = load_patch_data(
            patch_path, 
            parent_pcd_root=parent_pcd_root,
            grid_size=grid_size,
            verbose=True
        )
        
        patch_name = patch_data.get('name', patch_path.stem)
        complete_model_path = patch_data.get('_parent_pcd_path')
        gt_available = patch_data.get('gt_available', False)
        
        # ğŸ”¥ é¢„æµ‹
        print(f"\nâ³ æ¨¡å‹æ¨ç†...")
        pred_position_normalized = matcher.predict(patch_data, verbose=True)
        
        # ğŸ”¥ åå½’ä¸€åŒ–
        print(f"\nâ³ åå½’ä¸€åŒ–...")
        pred_position = denormalize_position(
            pred_position_normalized, 
            patch_data['norm_offset'],
            patch_data['norm_scale']
        )
        
        # ğŸ”¥ å¤„ç† GTï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
        gt_position = None
        gt_position_normalized = None
        error = None
        
        if gt_available and patch_data['gt_position'] is not None:
            gt_position_normalized = patch_data['gt_position']
            if isinstance(gt_position_normalized, torch.Tensor):
                gt_position_normalized = gt_position_normalized.cpu().numpy()
            else:
                gt_position_normalized = np.array(gt_position_normalized)
            
            gt_position = denormalize_position(
                gt_position_normalized,
                patch_data['norm_offset'],
                patch_data['norm_scale']
            )
            
            error = np.linalg.norm(pred_position - gt_position)
        
        # ğŸ”¥ æ‰“å°ç»“æœ
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ¨ç†ç»“æœ")
        print(f"{'='*80}")
        print(f"æ ·æœ¬åç§°:     {patch_name}")
        if complete_model_path:
            print(f"çˆ¶ç‚¹äº‘:       {complete_model_path.name}")
        
        print(f"\nå½’ä¸€åŒ–ç©ºé—´:")
        print(f"  é¢„æµ‹ä½ç½®:   [{pred_position_normalized[0]:.6f}, {pred_position_normalized[1]:.6f}, {pred_position_normalized[2]:.6f}]")
        if gt_position_normalized is not None:
            print(f"  GT ä½ç½®:    [{gt_position_normalized[0]:.6f}, {gt_position_normalized[1]:.6f}, {gt_position_normalized[2]:.6f}]")
        
        print(f"\nçœŸå®ç©ºé—´:")
        print(f"  é¢„æµ‹ä½ç½®:   [{pred_position[0]:.6f}, {pred_position[1]:.6f}, {pred_position[2]:.6f}] ç±³")
        if gt_position is not None:
            print(f"  GT ä½ç½®:    [{gt_position[0]:.6f}, {gt_position[1]:.6f}, {gt_position[2]:.6f}] ç±³")
        
        if error is not None:
            print(f"\nâœ… è¯¯å·®:")
            print(f"  {error:.6f} ç±³")
            print(f"  {error*1000:.2f} æ¯«ç±³")
        else:
            print(f"\nâš ï¸  æ—  GT ä½ç½®ï¼ˆçœŸå®æ¨ç†åœºæ™¯ï¼Œæ— æ³•è®¡ç®—è¯¯å·®ï¼‰")
        
        print(f"{'='*80}\n")
        
        # ğŸ”¥ ä¿å­˜ç»“æœ
        if save_dir is not None:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)
            
            result = {
                'patch_name': patch_name,
                'patch_path': str(patch_path),
                'pred_position_normalized': pred_position_normalized.tolist(),
                'pred_position': pred_position.tolist(),
                'gt_available': gt_available,
            }
            
            if complete_model_path:
                result['complete_model_path'] = str(complete_model_path)
            
            if gt_position is not None:
                result['gt_position_normalized'] = gt_position_normalized.tolist()
                result['gt_position'] = gt_position.tolist()
                result['error_meters'] = float(error)
                result['error_mm'] = float(error * 1000)
            
            json_path = save_dir / f"{patch_name}_result.json"
            with open(json_path, 'w') as f:
                json.dump(result, f, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜: {json_path}")
        
        # ğŸ”¥ å¯è§†åŒ–
        if complete_model_path and complete_model_path.exists():
            print(f"\nâ³ ç”Ÿæˆå¯è§†åŒ–...")
            visualize_prediction(
                patch_data,
                pred_position,
                gt_position,  # ğŸ”¥ å¯èƒ½æ˜¯ None
                complete_model_path,
                patch_name=patch_name,
                save_dir=save_dir,
                show_window=show_window
            )
        else:
            print(f"\nâš ï¸  æ— çˆ¶ç‚¹äº‘ï¼Œè·³è¿‡å¯è§†åŒ–")
        
        print(f"\n{'='*80}")
        print(f"âœ… å•æ ·æœ¬æ¨ç†å®Œæˆï¼")
        print(f"{'='*80}\n")
        
        return {
            'pred_position': pred_position,
            'gt_position': gt_position,
            'error_mm': error * 1000 if error is not None else None,
            'gt_available': gt_available,
            'patch_data': patch_data,
        }
        
    except Exception as e:
        print(f"\n{'='*80}")
        print(f"âŒ æ¨ç†å¤±è´¥")
        print(f"{'='*80}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_all_patches(
    matcher: PTv3ContactMatcher,
    dataset_dir: Path,
    category: str = "Scissors",
    save_dir: Path = Path("inference_results"),
    visualize_best_worst_median: bool = True,
    grid_size: float = 0.002,  # ğŸ”¥ æ–°å¢å‚æ•°
):
    """æµ‹è¯•æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ æ‰¹é‡æµ‹è¯•æ‰€æœ‰ç‚¹äº‘")
    print(f"{'='*80}")
    
    category_dir = dataset_dir / category / "patches"
    
    if not category_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {category_dir}")
        return
    
    patch_files = sorted(category_dir.glob("*.pth"))
    
    if not patch_files:
        print(f"âŒ ç›®å½•ä¸­æ²¡æœ‰ .pth æ–‡ä»¶: {category_dir}")
        return
    
    print(f"ğŸ“‚ æ•°æ®é›†ç›®å½•: {category_dir}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(patch_files)} ä¸ªå°ç‚¹äº‘")
    
    # æŸ¥æ‰¾æ‰€æœ‰å®Œæ•´ç‚¹äº‘æ–‡ä»¶
    complete_model_dir = dataset_dir / category
    complete_models = {}
    
    for ply_file in sorted(complete_model_dir.glob("bigpointcloud_*.ply")):
        model_id = ply_file.stem.split('_')[-1]
        complete_models[model_id] = ply_file
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(complete_models)} ä¸ªå¤§ç‚¹äº‘")
    if complete_models:
        print(f"   æ¨¡å‹ ID: {list(complete_models.keys())}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    batch_dir = save_dir / f"batch_{category}_{len(patch_files)}_samples"
    batch_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ç»“æœä¿å­˜ç›®å½•: {batch_dir}")
    
    # é€ä¸ªæµ‹è¯•
    results = []
    failed_samples = []
    
    print(f"\n{'='*80}")
    print(f"ğŸ”® å¼€å§‹æ‰¹é‡æ¨ç†...")
    print(f"{'='*80}\n")
    
    for i, patch_path in enumerate(patch_files):
        print(f"[{i+1}/{len(patch_files)}] å¤„ç†: {patch_path.name}")
        
        patch_name = patch_path.stem
        complete_model_path = None
        
        try:
            # ğŸ”¥ åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨æ–°çš„å‡½æ•°ï¼‰
            patch_data = load_patch_data(
                patch_path, 
                parent_pcd_root=dataset_dir,
                grid_size=grid_size,
                verbose=False
            )
            
            patch_name = patch_data.get('name', patch_path.stem)
            complete_model_path = patch_data['_parent_pcd_path']
            
            # é¢„æµ‹ï¼ˆå¾—åˆ°å½’ä¸€åŒ–åæ ‡ï¼‰
            pred_position_normalized = matcher.predict(patch_data, verbose=False)
            
            # GT ä½ç½®ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
            gt_position_normalized = patch_data['gt_position']
            if isinstance(gt_position_normalized, torch.Tensor):
                gt_position_normalized = gt_position_normalized.cpu().numpy()
            else:
                gt_position_normalized = np.array(gt_position_normalized)
            
            # ğŸ”¥ åå½’ä¸€åŒ–åˆ°çœŸå®ç©ºé—´
            pred_position = denormalize_position(
                pred_position_normalized, 
                patch_data['norm_offset'],
                patch_data['norm_scale']
            )
            gt_position = denormalize_position(
                gt_position_normalized,
                patch_data['norm_offset'],
                patch_data['norm_scale']
            )
            
            # è®¡ç®—è¯¯å·®ï¼ˆçœŸå®ç©ºé—´ï¼‰
            error = np.linalg.norm(pred_position - gt_position)
            
            # ä¿å­˜ç»“æœ
            result = {
                'index': i,
                'patch_name': patch_name,
                'patch_path': str(patch_path),
                'complete_model_path': str(complete_model_path),
                'complete_model_name': complete_model_path.name,
                'parent_id': patch_data['_raw_data'].get('parent_id', 'unknown'),
                'pred_position_normalized': pred_position_normalized.tolist(),
                'gt_position_normalized': gt_position_normalized.tolist(),
                'pred_position': pred_position.tolist(),
                'gt_position': gt_position.tolist(),
                'error_meters': float(error),
                'error_mm': float(error * 1000),
                'patch_data': patch_data,
            }
            results.append(result)
            
            print(f"   âœ… è¯¯å·®: {error*1000:.2f} mm")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            failed_samples.append({
                'patch_name': patch_name,
                'patch_path': str(patch_path),
                'reason': str(e)
            })
            continue
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ç»Ÿè®¡åˆ†æ")
    print(f"{'='*80}")
    
    if not results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„æ ·æœ¬ï¼")
        return
    
    errors = np.array([r['error_mm'] for r in results])
    
    print(f"æ€»æ ·æœ¬æ•°: {len(patch_files)}")
    print(f"æˆåŠŸæ ·æœ¬æ•°: {len(results)}")
    print(f"å¤±è´¥æ ·æœ¬æ•°: {len(failed_samples)}")
    print(f"\nè¯¯å·®ç»Ÿè®¡ (æ¯«ç±³):")
    print(f"  å¹³å‡å€¼:   {errors.mean():.2f} mm")
    print(f"  ä¸­ä½æ•°:   {np.median(errors):.2f} mm")
    print(f"  æ ‡å‡†å·®:   {errors.std():.2f} mm")
    print(f"  æœ€å°å€¼:   {errors.min():.2f} mm")
    print(f"  æœ€å¤§å€¼:   {errors.max():.2f} mm")
    
    # æ‰¾å‡ºæœ€å¥½ã€æœ€å·®ã€ä¸­ä½æ•°æ ·æœ¬
    best_idx = np.argmin(errors)
    worst_idx = np.argmax(errors)
    median_idx = np.argmin(np.abs(errors - np.median(errors)))
    
    best_sample = results[best_idx]
    worst_sample = results[worst_idx]
    median_sample = results[median_idx]
    
    print(f"\nğŸ† æœ€ä½³æ ·æœ¬: {best_sample['patch_name']} ({best_sample['error_mm']:.2f} mm)")
    print(f"ğŸ“‰ æœ€å·®æ ·æœ¬: {worst_sample['patch_name']} ({worst_sample['error_mm']:.2f} mm)")
    print(f"ğŸ“Š ä¸­ä½æ•°æ ·æœ¬: {median_sample['patch_name']} ({median_sample['error_mm']:.2f} mm)")
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    summary = {
        'category': category,
        'total_samples': len(patch_files),
        'successful_samples': len(results),
        'failed_samples': len(failed_samples),
        'statistics': {
            'mean_error_mm': float(errors.mean()),
            'median_error_mm': float(np.median(errors)),
            'std_error_mm': float(errors.std()),
            'min_error_mm': float(errors.min()),
            'max_error_mm': float(errors.max()),
        },
        'best_sample': {
            'name': best_sample['patch_name'],
            'error_mm': best_sample['error_mm'],
        },
        'worst_sample': {
            'name': worst_sample['patch_name'],
            'error_mm': worst_sample['error_mm'],
        },
        'median_sample': {
            'name': median_sample['patch_name'],
            'error_mm': median_sample['error_mm'],
        },
        'failed_samples': failed_samples,
    }
    
    summary_path = batch_dir / "summary.json"
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    print(f"\nâœ… ç»Ÿè®¡ç»“æœå·²ä¿å­˜: {summary_path}")
    
    # ä¿å­˜ CSV
    csv_path = batch_dir / "all_results.csv"
    with open(csv_path, 'w', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=[
            'index', 'patch_name', 'parent_id', 'complete_model_name',
            'pred_x', 'pred_y', 'pred_z',
            'gt_x', 'gt_y', 'gt_z',
            'error_mm'
        ])
        writer.writeheader()
        for r in results:
            writer.writerow({
                'index': r['index'],
                'patch_name': r['patch_name'],
                'parent_id': r['parent_id'],
                'complete_model_name': r['complete_model_name'],
                'pred_x': r['pred_position'][0],
                'pred_y': r['pred_position'][1],
                'pred_z': r['pred_position'][2],
                'gt_x': r['gt_position'][0],
                'gt_y': r['gt_position'][1],
                'gt_z': r['gt_position'][2],
                'error_mm': r['error_mm'],
            })
    print(f"âœ… CSV å·²ä¿å­˜: {csv_path}")
    
    # ç”Ÿæˆè¯¯å·®åˆ†å¸ƒå›¾
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        ax = axes[0, 0]
        ax.hist(errors, bins=50, color='skyblue', edgecolor='black', alpha=0.7)
        ax.axvline(errors.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {errors.mean():.2f} mm')
        ax.axvline(np.median(errors), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(errors):.2f} mm')
        ax.set_xlabel('Error (mm)')
        ax.set_ylabel('Frequency')
        ax.set_title('Error Distribution')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        ax = axes[0, 1]
        bp = ax.boxplot(errors, vert=True, patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        ax.set_ylabel('Error (mm)')
        ax.set_title('Error Boxplot')
        ax.grid(True, alpha=0.3)
        
        ax = axes[1, 0]
        ax.plot(range(len(errors)), errors, 'o-', markersize=2, linewidth=0.5, alpha=0.6)
        ax.axhline(errors.mean(), color='red', linestyle='--', linewidth=2, label='Mean')
        ax.axhline(np.median(errors), color='green', linestyle='--', linewidth=2, label='Median')
        ax.set_xlabel('Sample Index')
        ax.set_ylabel('Error (mm)')
        ax.set_title('Error vs Sample Index')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        ax = axes[1, 1]
        sorted_errors = np.sort(errors)
        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)
        ax.plot(sorted_errors, cumulative, linewidth=2)
        ax.axvline(errors.mean(), color='red', linestyle='--', linewidth=2, label='Mean')
        ax.axvline(np.median(errors), color='green', linestyle='--', linewidth=2, label='Median')
        ax.set_xlabel('Error (mm)')
        ax.set_ylabel('Cumulative Probability')
        ax.set_title('Cumulative Distribution Function')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.suptitle(f'Error Analysis - {category} ({len(results)} samples)', fontsize=16)
        plt.tight_layout()
        
        plot_path = batch_dir / "error_analysis.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è¯¯å·®åˆ†æå›¾å·²ä¿å­˜: {plot_path}")
    except Exception as e:
        print(f"âš ï¸  ç”Ÿæˆè¯¯å·®åˆ†æå›¾å¤±è´¥: {e}")
    
    # å¯è§†åŒ–ä»£è¡¨æ€§æ ·æœ¬
    if visualize_best_worst_median:
        print(f"\nğŸ¨ ç”Ÿæˆä»£è¡¨æ€§æ ·æœ¬çš„å¯è§†åŒ–\n")
        
        samples_to_visualize = [
            (best_sample, "best"),
            (median_sample, "median"),
            (worst_sample, "worst"),
        ]
        
        for sample, label in samples_to_visualize:
            try:
                visualize_prediction(
                    sample['patch_data'],
                    np.array(sample['pred_position']),
                    np.array(sample['gt_position']),
                    Path(sample['complete_model_path']),
                    patch_name=f"{label}_{sample['patch_name']}",
                    save_dir=batch_dir,
                    show_window=False
                )
            except Exception as e:
                print(f"   âš ï¸  å¯è§†åŒ–å¤±è´¥: {e}")
    
    print(f"\n{'='*80}")
    print(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {batch_dir}")
    print(f"{'='*80}\n")
    
    return results, summary

def main():
    parser = argparse.ArgumentParser(description='PTv3 Contact Position Regression æ¨ç†')
    parser.add_argument('--config', type=str, 
                        default='configs/s3dis/semseg-pt-v3m1-gelsight.py')
    parser.add_argument('--checkpoint', type=str,
                        default='exp/gelsight_test/model/model_best.pth')
    parser.add_argument('--dataset_dir', type=str,
                        default='../../touch_processed_data')
    parser.add_argument('--category', type=str,
                        default='Scissors',
                        choices=['Scissors', 'Cup', 'Avocado'])
    parser.add_argument('--save_dir', type=str,
                        default='inference_results')
    parser.add_argument('--no_vis', action='store_true',
                        help='ä¸ç”Ÿæˆå¯è§†åŒ–')
    parser.add_argument('--grid_size', type=float, default=0.002,
                        help='ä½“ç´ åŒ–ç½‘æ ¼å¤§å°(é»˜è®¤: 0.002)')
    parser.add_argument('--single', type=str, default=None,
                        help='å•æ ·æœ¬æ¨ç†ï¼šæŒ‡å®š .pth æ–‡ä»¶è·¯å¾„')   
    parser.add_argument('--no_window', action='store_true',
                        help='å•æ ·æœ¬æ¨¡å¼ï¼šä¸æ˜¾ç¤º Open3D çª—å£')     
    args = parser.parse_args()
    
    print(f"\n{'='*80}")
    print(f"ğŸš€ åˆå§‹åŒ– PTv3 Contact Matcher")
    print(f"{'='*80}")
    
    matcher = PTv3ContactMatcher(
        config_path=args.config,
        checkpoint_path=args.checkpoint
    )

    if args.single:
        test_single_patch(
            matcher=matcher,
            patch_path=Path(args.single),
            parent_pcd_root=Path(args.dataset_dir),
            save_dir=Path(args.save_dir) if args.save_dir else None,
            show_window=not args.no_window,
            grid_size=args.grid_size,
        )    
    else: 
        test_all_patches(
            matcher,
            dataset_dir=Path(args.dataset_dir),
            category=args.category,
            save_dir=Path(args.save_dir),
            visualize_best_worst_median=not args.no_vis,
            grid_size=args.grid_size,  # ğŸ”¥ ä¼ å…¥ grid_size
        )


if __name__ == "__main__":
    main()