import sys
import torch
from pathlib import Path
from torch.utils.data import DataLoader
from functools import partial

sys.path.insert(0, str(Path(__file__).parent.parent))

from pointcept.datasets.builder import build_dataset
from pointcept.datasets.utils import point_collate_fn
from pointcept.utils.config import Config


def print_separator(title="", width=80):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print("\n" + "=" * width)
        print(title.center(width))
        print("=" * width)
    else:
        print("=" * width)


def analyze_batch(batch, batch_idx):
    """è¯¦ç»†åˆ†æä¸€ä¸ª batch çš„å†…å®¹"""
    print(f"\n{'â”€' * 80}")
    print(f"ğŸ“¦ Batch {batch_idx} Analysis")
    print(f"{'â”€' * 80}")
    
    print(f"\n1ï¸âƒ£  åŸºæœ¬ä¿¡æ¯:")
    print(f"   - Type: {type(batch)}")
    print(f"   - Keys: {list(batch.keys())}")
    
    # ğŸ”¥ åˆ†ææ‰€æœ‰å¼ é‡
    print(f"\n2ï¸âƒ£  å¼ é‡å½¢çŠ¶:")
    tensor_keys = ['coord', 'grid_coord', 'feat', 'offset', 'gt_position', 
                   'category_id', 'batch', 'parent_coord', 'parent_color',
                   'norm_offset', 'norm_scale']
    
    for key in tensor_keys:
        if key in batch:
            value = batch[key]
            if isinstance(value, torch.Tensor):
                print(f"   âœ… {key:20s}: shape={str(value.shape):20s} dtype={value.dtype}")
            elif isinstance(value, list):
                print(f"   âœ… {key:20s}: list of {len(value)} items")
            else:
                print(f"   âš ï¸  {key:20s}: {type(value)}")
    
    # ğŸ”¥ åˆ†æ offsetï¼ˆå±€éƒ¨ç‚¹äº‘çš„åˆ†å‰²ï¼‰
    if "offset" in batch:
        offset = batch["offset"]
        print(f"\n3ï¸âƒ£  Offset åˆ†æ (å±€éƒ¨ç‚¹äº‘åˆ†å‰²):")
        print(f"   - Offset tensor: {offset}")
        print(f"   - Batch size: {len(offset)}")
        
        print(f"\n   å„æ ·æœ¬çš„å±€éƒ¨ç‚¹äº‘ç‚¹æ•°:")
        print(f"      Sample 0: {offset[0]:6d} points")
        for i in range(1, len(offset)):
            n_points = offset[i] - offset[i - 1]
            print(f"      Sample {i}: {n_points:6d} points")
        
        total_local_points = offset[-1].item() if len(offset) > 0 else 0
        print(f"\n   æ€»å±€éƒ¨ç‚¹æ•°: {total_local_points}")
    
    # ğŸ”¥ åˆ†æçˆ¶ç‚¹äº‘
    if "parent_coord" in batch:
        parent_coord = batch["parent_coord"]
        print(f"\n4ï¸âƒ£  çˆ¶ç‚¹äº‘åˆ†æ:")
        print(f"   - parent_coord shape: {parent_coord.shape}")
        print(f"   - parent_color shape: {batch['parent_color'].shape if 'parent_color' in batch else 'N/A'}")
        
        if "batch" in batch:
            batch_idx_tensor = batch["batch"]
            
            # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬çš„çˆ¶ç‚¹äº‘ç‚¹æ•°
            unique_batches = torch.unique(batch_idx_tensor)
            print(f"\n   å„æ ·æœ¬çš„çˆ¶ç‚¹äº‘ç‚¹æ•°:")
            for b_idx in unique_batches:
                mask = batch_idx_tensor == b_idx
                n_parent_points = mask.sum().item()
                print(f"      Sample {b_idx}: {n_parent_points:6d} points")
            
            print(f"\n   æ€»çˆ¶ç‚¹äº‘ç‚¹æ•°: {len(batch_idx_tensor)}")
    
    # ğŸ”¥ åˆ†æå½’ä¸€åŒ–å‚æ•°
    if "norm_offset" in batch or "norm_scale" in batch:
        print(f"\n5ï¸âƒ£  å½’ä¸€åŒ–å‚æ•°:")
        if "norm_offset" in batch:
            norm_offset = batch["norm_offset"]
            print(f"   - norm_offset shape: {norm_offset.shape}")
            if norm_offset.dim() == 2:
                for i in range(norm_offset.shape[0]):
                    print(f"      Sample {i}: {norm_offset[i].tolist()}")
        
        if "norm_scale" in batch:
            norm_scale = batch["norm_scale"]
            print(f"   - norm_scale shape: {norm_scale.shape}")
            if norm_scale.dim() == 1:
                for i in range(norm_scale.shape[0]):
                    print(f"      Sample {i}: {norm_scale[i].item():.6f}")
    
    # ğŸ”¥ åˆ†æ GT
    if "gt_position" in batch:
        gt_position = batch["gt_position"]
        print(f"\n6ï¸âƒ£  Ground Truth:")
        print(f"   - gt_position shape: {gt_position.shape}")
        for j in range(gt_position.shape[0]):
            print(f"      Sample {j}: [{gt_position[j, 0]:.6f}, {gt_position[j, 1]:.6f}, {gt_position[j, 2]:.6f}]")
    
    # ğŸ”¥ åˆ†æç±»åˆ«
    if "category_id" in batch:
        category_id = batch["category_id"]
        print(f"\n7ï¸âƒ£  ç±»åˆ« ID:")
        print(f"   - category_id shape: {category_id.shape}")
        
        category_names = {0: "Scissors", 1: "Cup", 2: "Avocado"}
        for j in range(category_id.shape[0]):
            cat_id = category_id[j].item()
            cat_name = category_names.get(cat_id, "Unknown")
            print(f"      Sample {j}: {cat_id} ({cat_name})")
    
    # ğŸ”¥ åˆ†ææ ·æœ¬åç§°
    if "name" in batch:
        print(f"\n8ï¸âƒ£  æ ·æœ¬åç§°:")
        for j, name in enumerate(batch['name']):
            print(f"      Sample {j}: {name}")
    
    print(f"\n{'â”€' * 80}\n")


def test_train_dataloader():
    """æµ‹è¯•è®­ç»ƒé›† DataLoader"""
    print_separator("ğŸš‚ æµ‹è¯•è®­ç»ƒé›† DataLoader")
    
    cfg = Config.fromfile("/home/ypf/PointTransformerV3_Simple/configs/s3dis/semseg-pt-v3m1-gelsight.py")
    
    print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒé›†...")
    train_dataset = build_dataset(cfg.data.train)
    
    print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"      - æ•°æ®é›†ç±»å‹: {type(train_dataset).__name__}")
    print(f"      - æ ·æœ¬æ•°é‡: {len(train_dataset)}")
    print(f"      - Split: {train_dataset.split}")
    print(f"      - Parent cache size: {train_dataset.max_cache_size}")
    
    batch_size = 4
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=partial(point_collate_fn, mix_prob=0.0),
        pin_memory=False,
    )
    
    print(f"\nğŸ“¦ åˆ›å»º DataLoader:")
    print(f"   âœ… DataLoader åˆ›å»ºæˆåŠŸ!")
    print(f"      - Batch size: {batch_size}")
    print(f"      - Total batches: {len(train_loader)}")
    print(f"      - Collate function: point_collate_fn (mix_prob=0.0)")
    
    print_separator("ğŸ” æµ‹è¯•å‰ 2 ä¸ª Batch")
    
    for i, batch in enumerate(train_loader):
        if i >= 2:
            break
        
        analyze_batch(batch, i)
    
    print_separator("âœ… è®­ç»ƒé›† DataLoader æµ‹è¯•å®Œæˆ")


def test_val_dataloader():
    """æµ‹è¯•éªŒè¯é›† DataLoader"""
    print_separator("ğŸ” æµ‹è¯•éªŒè¯é›† DataLoader")
    
    cfg = Config.fromfile("/home/ypf/PointTransformerV3_Simple/configs/s3dis/semseg-pt-v3m1-gelsight.py")
    
    print(f"\nğŸ“‚ åŠ è½½éªŒè¯é›†...")
    val_dataset = build_dataset(cfg.data.val)
    
    print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"      - æ•°æ®é›†ç±»å‹: {type(val_dataset).__name__}")
    print(f"      - æ ·æœ¬æ•°é‡: {len(val_dataset)}")
    print(f"      - Split: {val_dataset.split}")
    
    batch_size = 2
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=partial(point_collate_fn, mix_prob=0.0),
        pin_memory=False,
    )
    
    print(f"\nğŸ“¦ åˆ›å»º DataLoader:")
    print(f"   âœ… DataLoader åˆ›å»ºæˆåŠŸ!")
    print(f"      - Batch size: {batch_size}")
    print(f"      - Total batches: {len(val_loader)}")
    
    print_separator("ğŸ” æµ‹è¯•å‰ 1 ä¸ª Batch")
    
    for i, batch in enumerate(val_loader):
        if i >= 1:
            break
        
        analyze_batch(batch, i)
    
    print_separator("âœ… éªŒè¯é›† DataLoader æµ‹è¯•å®Œæˆ")


def test_test_dataloader():
    """æµ‹è¯•æµ‹è¯•é›† DataLoaderï¼ˆæ³¨æ„ï¼šæµ‹è¯•é›†è¿”å›çš„æ•°æ®ç»“æ„ä¸åŒï¼‰"""
    print_separator("ğŸ§ª æµ‹è¯•æµ‹è¯•é›† DataLoader")
    
    cfg = Config.fromfile("/home/ypf/PointTransformerV3_Simple/configs/s3dis/semseg-pt-v3m1-gelsight.py")
    
    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•é›†...")
    test_dataset = build_dataset(cfg.data.test)
    
    print(f"   âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
    print(f"      - æ•°æ®é›†ç±»å‹: {type(test_dataset).__name__}")
    print(f"      - æ ·æœ¬æ•°é‡: {len(test_dataset)}")
    print(f"      - Split: {test_dataset.split}")
    print(f"      - Test mode: {test_dataset.test_mode}")
    
    print(f"\nâš ï¸  æ³¨æ„ï¼šæµ‹è¯•é›†è¿”å›çš„æ•°æ®ç»“æ„ä¸åŒï¼ˆåŒ…å« fragment_listï¼‰")
    
    # æµ‹è¯•å•ä¸ªæ ·æœ¬
    print_separator("ğŸ” æµ‹è¯•å•ä¸ªæ ·æœ¬")
    
    sample = test_dataset[0]
    
    print(f"\nğŸ“¦ Sample 0:")
    print(f"   - Type: {type(sample)}")
    print(f"   - Outer keys: {list(sample.keys())}")
    
    # ğŸ”¥ åˆ†æå¤–å±‚å­—æ®µ
    print(f"\n1ï¸âƒ£  å¤–å±‚å­—æ®µ:")
    for key in ['gt_position', 'name', 'category_id', 'parent_coord', 
                'parent_color', 'norm_offset', 'norm_scale']:
        if key in sample:
            value = sample[key]
            if isinstance(value, torch.Tensor):
                print(f"   âœ… {key:20s}: shape={value.shape}, dtype={value.dtype}")
            elif isinstance(value, np.ndarray):
                print(f"   âœ… {key:20s}: shape={value.shape}, dtype={value.dtype}")
            elif isinstance(value, (int, float)):
                print(f"   âœ… {key:20s}: {value}")
            else:
                print(f"   âœ… {key:20s}: {type(value)}")
    
    # ğŸ”¥ åˆ†æ fragment_list
    if 'fragment_list' in sample:
        print(f"\n2ï¸âƒ£  Fragment List:")
        print(f"   - Number of fragments: {len(sample['fragment_list'])}")
        
        for i, fragment in enumerate(sample['fragment_list']):
            print(f"\n   Fragment {i}:")
            print(f"      Keys: {list(fragment.keys())}")
            
            for key in ['coord', 'grid_coord', 'feat', 'index', 'name',
                       'parent_coord', 'parent_color', 'norm_offset', 'norm_scale']:
                if key in fragment:
                    value = fragment[key]
                    if isinstance(value, torch.Tensor):
                        print(f"      - {key:20s}: shape={value.shape}")
    
    print_separator("âœ… æµ‹è¯•é›† DataLoader æµ‹è¯•å®Œæˆ")


def test_collate_fn():
    """æµ‹è¯• collate_fn æ˜¯å¦æ­£ç¡®å¤„ç†çˆ¶ç‚¹äº‘"""
    print_separator("ğŸ”§ æµ‹è¯• point_collate_fn")
    
    cfg = Config.fromfile("/home/ypf/PointTransformerV3_Simple/configs/s3dis/semseg-pt-v3m1-gelsight.py")
    
    print(f"\nğŸ“‚ åŠ è½½è®­ç»ƒé›†...")
    train_dataset = build_dataset(cfg.data.train)
    
    # æ‰‹åŠ¨è·å–å‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“¦ æ‰‹åŠ¨è·å– 2 ä¸ªæ ·æœ¬...")
    samples = [train_dataset[i] for i in range(2)]
    
    print(f"\n   Sample 0 keys: {list(samples[0].keys())}")
    print(f"   Sample 1 keys: {list(samples[1].keys())}")
    
    # ä½¿ç”¨ collate_fn
    print(f"\nğŸ”§ è°ƒç”¨ point_collate_fn...")
    batch = point_collate_fn(samples, mix_prob=0.0)
    
    print(f"\n   âœ… Collate æˆåŠŸ!")
    print(f"   Batch keys: {list(batch.keys())}")
    
    # åˆ†æç»“æœ
    analyze_batch(batch, 0)
    
    print_separator("âœ… point_collate_fn æµ‹è¯•å®Œæˆ")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    import numpy as np  # éœ€è¦åœ¨è¿™é‡Œå¯¼å…¥ï¼ˆç”¨äºæµ‹è¯•é›†åˆ†æï¼‰
    
    print("\n" + "ğŸš€" * 40)
    print("å¼€å§‹æµ‹è¯•å¸¦çˆ¶ç‚¹äº‘çš„ DataLoader".center(80))
    print("ğŸš€" * 40)
    
    try:
        # 1. æµ‹è¯•è®­ç»ƒé›†
        test_train_dataloader()
        
        # 2. æµ‹è¯•éªŒè¯é›†
        test_val_dataloader()
        
        # 3. æµ‹è¯• collate_fn
        test_collate_fn()
        
        # 4. æµ‹è¯•æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰
        # test_test_dataloader()
        
        print("\n" + "ğŸ‰" * 40)
        print("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼".center(80))
        print("ğŸ‰" * 40 + "\n")
        
    except Exception as e:
        print("\n" + "âŒ" * 40)
        print(f"æµ‹è¯•å¤±è´¥: {e}".center(80))
        print("âŒ" * 40 + "\n")
        raise


if __name__ == "__main__":
    main()